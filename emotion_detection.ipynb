{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# All skl imports go here\n",
    "from sklearn import tree   # Decision Trees\n",
    "from sklearn import svm    # svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import sklearn as skl\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "from neuralnet import NeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 6\n",
    "CLASSES = [\"sadnesss\", \"joy\", \"love\", \"anger\", \"fear\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data\n",
    "train_data = pd.read_csv(\"data/training_labse.csv\")\n",
    "test_data = pd.read_csv(\"data/test_labse.csv\")\n",
    "validation_data = pd.read_csv(\"data/validation_labse.csv\")\n",
    "\n",
    "# Separate X's and y's from each other\n",
    "FEATURE_COLUMNS = [x for x in train_data if x.startswith(\"_e\")]\n",
    "LABEL_COLUMN = \"label\"\n",
    "\n",
    "X_train = train_data[FEATURE_COLUMNS]\n",
    "Y_train = train_data[LABEL_COLUMN]\n",
    "\n",
    "X_test = test_data[FEATURE_COLUMNS]\n",
    "Y_test = test_data[LABEL_COLUMN]\n",
    "\n",
    "X_val = validation_data[FEATURE_COLUMNS]\n",
    "Y_val = validation_data[LABEL_COLUMN]\n",
    "\n",
    "# These are used to run cross validation\n",
    "X_train_val = pd.concat([X_train, X_val]) \n",
    "Y_train_val = pd.concat([Y_train, Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform pre-processing PCA on the training set\n",
    "TARGET_EXPLAINED_VARIANCE = 0.95 \n",
    "\n",
    "def perform_pca(dataset):\n",
    "    pca = PCA(n_components= TARGET_EXPLAINED_VARIANCE)\n",
    "\n",
    "    # Need to standardize the data frirst\n",
    "    standardized = (dataset - dataset.mean(axis=0)) / dataset.std(axis = 0)\n",
    "\n",
    "    pca.fit(X=standardized)\n",
    "    dataset_reduced = pca.fit_transform(X=standardized)\n",
    "\n",
    "    return pca, dataset_reduced\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_train, X_train_reduced = perform_pca(X_train)\n",
    "pca_tran_val, X_train_val_reduced = perform_pca(X_train_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Folds Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def k_folds_x_val(model): \n",
    "    cumulative_accuracy = 0\n",
    "    cumulative_kappa = 0\n",
    "\n",
    "    k_folds = KFold(n_splits=10)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(k_folds.split(X_train_val_reduced, Y_train_val)):\n",
    "        model.fit(X_train_val_reduced[train_index], Y_train_val.iloc[train_index])\n",
    "        \n",
    "        Y_pred = model.predict(X_train_val_reduced[test_index])\n",
    "        Y_true = Y_train_val.iloc[test_index]\n",
    "        # Add all metrics here\n",
    "\n",
    "        cumulative_accuracy += metrics.accuracy_score(Y_true, Y_pred)\n",
    "        cumulative_kappa += metrics.cohen_kappa_score(Y_true, Y_pred)\n",
    "\n",
    "    folds = k_folds.get_n_splits()\n",
    "    print(f\"Performed {folds}-fold cross validation\")\n",
    "    print(f\"Average accuracy {cumulative_accuracy / folds}\")\n",
    "    print(f\"Average Kappa {cumulative_kappa / folds}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridSearchHPO(model, search_space):\n",
    "    grid_search = GridSearchCV(estimator=model,\n",
    "                                param_grid=search_space,\n",
    "                                scoring='accuracy',\n",
    "                                cv=5,\n",
    "                                verbose=3,\n",
    "                                error_score='raise',\n",
    "                                n_jobs=-1,  # -1 means max amount\n",
    "                                )\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model = tree.DecisionTreeClassifier(\n",
    "    criterion ='entropy', \n",
    "    splitter = 'best',\n",
    "    max_depth = 256, \n",
    "    max_features = 'sqrt',\n",
    "    max_leaf_nodes = 80,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', \n",
    "# 'min_impurity_decrease', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'random_state', 'splitter']\n",
    "dt_search_space = {\n",
    "    # 'ccp_alpha':[0.1, 0.2, 0.4, 0.5],\n",
    "    'max_leaf_nodes':[71, 72, 73, 74, 75, 76, 77, 78, 79, 80], \n",
    "    # 'min_impurity_decrease':[1.0, 0.5, 1.5, 2.0], # float\n",
    "    # 'min_weight_fraction_leaf':[0.1, 0.2, 0.4, 0.5],\n",
    "    'max_depth':[255, 256, 257, 258, 259, 260], \n",
    "    # 'max_features':[\"log2\", \"sqrt\"], \n",
    "    # 'min_samples_leaf':[0.1, 0.2, 0.4, 0.5], \n",
    "    # 'min_samples_split':[0.1, 0.2, 0.4, 0.5]\n",
    "    }\n",
    "\n",
    "model_dt = decision_tree_model.fit(X_train, Y_train)\n",
    "\n",
    "gridsearch_dt = gridSearchHPO(model=model_dt, search_space=dt_search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_dt.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Score: {}\".format(gridsearch_dt.best_score_))\n",
    "print(\"Best params: {}\".format(gridsearch_dt.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = svm.SVC(max_iter=-1)\n",
    "\n",
    "#Train the model using the training sets\n",
    "svm_classifier.fit(X_train, Y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "# k_folds_x_val(svm_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_search_space={\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'kernel': ['poly'], # poly - 'rbf', 'linear', 'sigmoid' \n",
    "    'degree': [3],  # 3\n",
    "    'gamma': ['scale'], #'auto',  1, 0.1, 0.01, 0.001, 0.0001, \n",
    "    'coef0': [0.0, 0.1, 0.2, 0.5, 0.8, 1.0], \n",
    "    'shrinking': [True], \n",
    "    'probability': [False], \n",
    "    'tol': [0.1],  \n",
    "    'class_weight': [None], \n",
    "    'decision_function_shape': ['ovr'], # 'ovo'\n",
    "}\n",
    "\n",
    "gridsearch_svm = gridSearchHPO(svm_classifier, svm_search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_svm.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Score: {}\".format(gridsearch_svm.best_score_))\n",
    "print(\"Best params: {}\".format(gridsearch_svm.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch specific constants\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from utils.neuralnet import NeuralNetwork\n",
    "from utils.trainer import training_loop, evaluate\n",
    "import torch\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pytorch specifically we should load data to the provided dataloader and dataset classes. \n",
    "# This handles the batching for us.\n",
    "\n",
    "pt_train_set = TensorDataset(torch.Tensor(X_train_reduced), torch.Tensor(Y_train.to_numpy()).long())\n",
    "pt_val_set = TensorDataset(torch.Tensor(pca_train.transform(X_val)), torch.Tensor(Y_val.to_numpy()).long())\n",
    "pt_test_set = TensorDataset(torch.Tensor(pca_train.transform(X_test)), torch.Tensor(Y_test.to_numpy()).long())\n",
    "pt_val_test_set = TensorDataset(torch.Tensor(pca_train.transform(X_val_test)), torch.Tensor(Y_val_test.to_numpy()).long())\n",
    "\n",
    "pt_train_loader = DataLoader(\n",
    "                    dataset=pt_train_set, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    shuffle=True,\n",
    "                )\n",
    "pt_val_loader = DataLoader(\n",
    "                    dataset=pt_val_set, \n",
    "                    batch_size=1, \n",
    "                    shuffle=True,\n",
    "                )\n",
    "pt_test_loader = DataLoader(\n",
    "                    dataset=pt_test_set, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    shuffle=True,\n",
    "                )\n",
    "\n",
    "pt_val_test_loader = DataLoader(\n",
    "                    dataset=pt_val_test_set, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    shuffle=True,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize neural network\n",
    "mlp = NeuralNetwork(X_train_reduced.shape[1], [], NUM_CLASSES)\n",
    "\n",
    "training_loop(mlp, pt_train_loader, pt_val_loader, 10, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model=mlp, val_dl=pt_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Only Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add stuff for running the test set on the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
