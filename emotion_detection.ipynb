{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# All skl imports go here\n",
    "from sklearn import tree   # Decision Trees\n",
    "from sklearn import svm    # svm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import sklearn as skl\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "from neuralnet import NeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 6\n",
    "CLASSES = [\"sadnesss\", \"joy\", \"love\", \"anger\", \"fear\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data\n",
    "train_data = pd.read_csv(\"training_labse.csv\")\n",
    "test_data = pd.read_csv(\"test_labse.csv\")\n",
    "validation_data = pd.read_csv(\"validation_labse.csv\")\n",
    "\n",
    "# Separate X's and y's from each other\n",
    "FEATURE_COLUMNS = [x for x in train_data if x.startswith(\"_e\")]\n",
    "LABEL_COLUMN = \"label\"\n",
    "\n",
    "X_train = train_data[FEATURE_COLUMNS]\n",
    "Y_train = train_data[LABEL_COLUMN]\n",
    "\n",
    "X_test = test_data[FEATURE_COLUMNS]\n",
    "Y_test = test_data[LABEL_COLUMN]\n",
    "\n",
    "X_val = validation_data[FEATURE_COLUMNS]\n",
    "Y_val = validation_data[LABEL_COLUMN]\n",
    "\n",
    "# These are used to run cross validation\n",
    "X_train_val = pd.concat([X_train, X_val]) \n",
    "Y_train_val = pd.concat([Y_train, Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform pre-processing PCA on the training set\n",
    "TARGET_EXPLAINED_VARIANCE = 0.95 \n",
    "\n",
    "def perform_pca(dataset):\n",
    "    pca = PCA(n_components= TARGET_EXPLAINED_VARIANCE)\n",
    "\n",
    "    # Need to standardize the data frirst\n",
    "    standardized = (dataset - dataset.mean(axis=0)) / dataset.std(axis = 0)\n",
    "\n",
    "    pca.fit(X=standardized)\n",
    "    dataset_reduced = pca.fit_transform(X=standardized)\n",
    "\n",
    "    return pca, dataset_reduced\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_train, X_train_reduced = perform_pca(X_train)\n",
    "pca_tran_val, X_train_val_reduced = perform_pca(X_train_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Folds Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def k_folds_x_val(model): \n",
    "    cumulative_accuracy = 0\n",
    "    cumulative_kappa = 0\n",
    "\n",
    "    k_folds = KFold(n_splits=10)\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(k_folds.split(X_train_val_reduced, Y_train_val)):\n",
    "        model.fit(X_train_val_reduced[train_index], Y_train_val.iloc[train_index])\n",
    "        \n",
    "        Y_pred = model.predict(X_train_val_reduced[test_index])\n",
    "        Y_true = Y_train_val.iloc[test_index]\n",
    "        # Add all metrics here\n",
    "\n",
    "        cumulative_accuracy += metrics.accuracy_score(Y_true, Y_pred)\n",
    "        cumulative_kappa += metrics.cohen_kappa_score(Y_true, Y_pred)\n",
    "\n",
    "    folds = k_folds.get_n_splits()\n",
    "    print(f\"Performed {folds}-fold cross validation\")\n",
    "    print(f\"Average accuracy {cumulative_accuracy / folds}\")\n",
    "    print(f\"Average Kappa {cumulative_kappa / folds}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(model):\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "    return metrics.accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "def gridSearchHPO(model, search_space):\n",
    "    grid_search = GridSearchCV(estimator=model, \n",
    "                                param_grid=search_space, \n",
    "                                scoring=objective,\n",
    "                                cv=5,\n",
    "                                n_jobs=-1) # -1 means max amount\n",
    "    grid_search.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model = tree.DecisionTreeClassifier(\n",
    "    criterion='entropy', \n",
    "    splitter = 'best',\n",
    "    max_depth = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds_x_val(decision_tree_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model.fit(X_train_reduced, Y_train)\n",
    "tree.plot_tree(decision_tree_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = svm.SVC(kernel='sigmoid') # 'precomputed', 'linear', 'poly', 'sigmoid', 'rbf'\n",
    "\n",
    "#Train the model using the training sets\n",
    "svm_classifier.fit(X_train_reduced, Y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "k_folds_x_val(svm_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch specific constants\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pytorch specifically we should load data to the provided dataloader and dataset classes. \n",
    "# This handles the batching for us.\n",
    "\n",
    "pt_train_set = TensorDataset(torch.Tensor(X_train_reduced), torch.Tensor(Y_train.to_numpy()).long())\n",
    "pt_val_set = TensorDataset(torch.Tensor(pca_train.transform(X_val)), torch.Tensor(Y_val.to_numpy()).long())\n",
    "pt_test_set = TensorDataset(torch.Tensor(pca_train.transform(X_test)), torch.Tensor(Y_test.to_numpy()).long())\n",
    "\n",
    "pt_train_loader = DataLoader(\n",
    "                    dataset=pt_train_set, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    shuffle=True,\n",
    "                )\n",
    "pt_val_loader = DataLoader(\n",
    "                    dataset=pt_val_set, \n",
    "                    batch_size=1, \n",
    "                    shuffle=True,\n",
    "                )\n",
    "pt_test_loader = DataLoader(\n",
    "                    dataset=pt_test_set, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    shuffle=True,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model: torch.nn.modules, dl: DataLoader, optimizer, loss_fn):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(dl):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        last_loss = loss.item()\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "def training_loop(model : torch.nn.Module, dl : DataLoader, epochs = 1):\n",
    "    optimizer = Adam(params=model.parameters(True), lr=LEARNING_RATE, betas=[0.9, 0.999])\n",
    "    loss_fn = CrossEntropyLoss()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "\n",
    "        model.train()\n",
    "        train_loss = train_one_epoch(model, dl, optimizer, loss_fn)\n",
    "\n",
    "        model.eval()\n",
    "        running_vloss = 0\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(pt_val_loader):\n",
    "                vinputs, vlabels = vdata\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "                running_vloss += vloss\n",
    "\n",
    "        avg_vloss = running_vloss / len(pt_val_loader)\n",
    "        print(f\"train_loss = {train_loss :.4f}, val_loss = {avg_vloss :.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize neural network\n",
    "mlp = NeuralNetwork(X_train_reduced.shape[1], [], NUM_CLASSES)\n",
    "\n",
    "training_loop(mlp, pt_train_loader, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Only Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_test = decision_tree_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add stuff for running the test set on the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
