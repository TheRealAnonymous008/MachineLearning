{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 6\n",
    "CLASSES = [\"sadnesss\", \"joy\", \"love\", \"anger\", \"fear\"]\n",
    "WEIGHTS = torch.tensor([0.00287505, 0.00246512, 0.01015641, 0.00615233, 0.00702346, 0.02318034])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data\n",
    "train_data = pd.read_csv(\"data/training.csv\")\n",
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "validation_data = pd.read_csv(\"data/validation.csv\")\n",
    "\n",
    "# Separate X's and y's from each other\n",
    "FEATURE_COLUMNS = [\"text\"]\n",
    "LABEL_COLUMN = \"label\"\n",
    "\n",
    "X_train = train_data[FEATURE_COLUMNS]\n",
    "Y_train = train_data[LABEL_COLUMN]\n",
    "\n",
    "X_test = test_data[FEATURE_COLUMNS]\n",
    "Y_test = test_data[LABEL_COLUMN]\n",
    "\n",
    "X_val = validation_data[FEATURE_COLUMNS]\n",
    "Y_val = validation_data[LABEL_COLUMN]\n",
    "\n",
    "# These are used to run cross validation\n",
    "X_train_val = pd.concat([X_train, X_val]) \n",
    "Y_train_val = pd.concat([Y_train, Y_test])\n",
    "\n",
    "# These are used to run val and test for Neural Nets\n",
    "X_val_test = pd.concat([X_val, X_test])\n",
    "Y_val_test = pd.concat([Y_val, Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "X_train_maxtoks = X_train['text'].str.len().max()\n",
    "X_test_maxtoks = X_test['text'].str.len().max()\n",
    "X_val_maxtoks = X_val['text'].str.len().max() \n",
    "\n",
    "max_toks = max(X_train_maxtoks, X_test_maxtoks, X_val_maxtoks)\n",
    "print(max_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch specific constants\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from extractors.chartok import  CharTokenDataset\n",
    "from utils.neuralnet import NeuralNetwork\n",
    "from utils.rnn import LSTMNetwork\n",
    "from utils.transformer import TransformerEncoder\n",
    "from utils.trainer import training_loop, evaluate\n",
    "import torch\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard 1 Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of utils.transformer failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Joaquin\\miniconda3\\envs\\agienv\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\Joaquin\\miniconda3\\envs\\agienv\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"c:\\Users\\Joaquin\\miniconda3\\envs\\agienv\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"c:\\Users\\Joaquin\\miniconda3\\envs\\agienv\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 365, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"c:\\Users\\Joaquin\\miniconda3\\envs\\agienv\\Lib\\site-packages\\IPython\\extensions\\autoreload.py\", line -1, in update_instances\n",
      "KeyboardInterrupt\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Because an MLP operates on fixed size inputs, we will use the entire fixed size input for this\n",
    "train_data = CharTokenDataset(X_train, Y_train, tokenizer=None, max_seq_length=max_toks)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_data = CharTokenDataset(X_val, Y_val, tokenizer=None, max_seq_length=max_toks)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Batch size is hard coded to 1 for accuracy purposes. \n",
    "test_data = CharTokenDataset(X_test, Y_test, tokenizer=None, max_seq_length=max_toks)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "train_loss = 13.9535, val_loss = 10.8022\n",
      "Epoch 2\n",
      "train_loss = 9.9314, val_loss = 9.3420\n",
      "Epoch 3\n",
      "train_loss = 6.8273, val_loss = 8.0207\n",
      "Epoch 4\n",
      "train_loss = 7.8311, val_loss = 6.9920\n",
      "Epoch 5\n",
      "train_loss = 4.5947, val_loss = 5.9817\n",
      "Epoch 6\n",
      "train_loss = 4.9756, val_loss = 5.2777\n",
      "Epoch 7\n",
      "train_loss = 5.8346, val_loss = 4.4615\n",
      "Epoch 8\n",
      "train_loss = 3.2274, val_loss = 3.8887\n",
      "Epoch 9\n",
      "train_loss = 4.2113, val_loss = 3.4857\n",
      "Epoch 10\n",
      "train_loss = 2.4747, val_loss = 3.0273\n",
      "Epoch 11\n",
      "train_loss = 2.4019, val_loss = 2.7825\n",
      "Epoch 12\n",
      "train_loss = 2.0958, val_loss = 2.6051\n",
      "Epoch 13\n",
      "train_loss = 3.0529, val_loss = 2.4993\n",
      "Epoch 14\n",
      "train_loss = 1.9323, val_loss = 2.2548\n",
      "Epoch 15\n",
      "train_loss = 2.1267, val_loss = 2.2779\n",
      "Epoch 16\n",
      "train_loss = 1.8620, val_loss = 2.3167\n",
      "Epoch 17\n",
      "train_loss = 1.4648, val_loss = 2.1148\n",
      "Epoch 18\n",
      "train_loss = 1.6963, val_loss = 2.1023\n",
      "Epoch 19\n",
      "train_loss = 2.2139, val_loss = 1.9956\n",
      "Epoch 20\n",
      "train_loss = 1.7367, val_loss = 1.9880\n",
      "Epoch 21\n",
      "train_loss = 1.5239, val_loss = 2.0146\n",
      "Epoch 22\n",
      "train_loss = 1.8472, val_loss = 1.9583\n",
      "Epoch 23\n",
      "train_loss = 1.9749, val_loss = 1.9652\n",
      "Epoch 24\n",
      "train_loss = 1.5908, val_loss = 1.8599\n",
      "Epoch 25\n",
      "train_loss = 1.4767, val_loss = 1.8390\n",
      "Epoch 26\n",
      "train_loss = 2.0671, val_loss = 2.0057\n",
      "Epoch 27\n",
      "train_loss = 1.6085, val_loss = 1.8656\n",
      "Epoch 28\n",
      "train_loss = 1.9076, val_loss = 1.9487\n",
      "Epoch 29\n",
      "train_loss = 1.6700, val_loss = 1.8162\n",
      "Epoch 30\n",
      "train_loss = 1.6858, val_loss = 1.8054\n",
      "Epoch 31\n",
      "train_loss = 1.6059, val_loss = 1.7851\n",
      "Epoch 32\n",
      "train_loss = 1.9667, val_loss = 1.8047\n",
      "Epoch 33\n",
      "train_loss = 1.7357, val_loss = 1.8102\n",
      "Epoch 34\n",
      "train_loss = 1.6395, val_loss = 1.8423\n",
      "Epoch 35\n",
      "train_loss = 1.3900, val_loss = 1.8502\n",
      "Epoch 36\n",
      "train_loss = 1.5131, val_loss = 1.8085\n",
      "Epoch 37\n",
      "train_loss = 1.6660, val_loss = 1.8523\n",
      "Epoch 38\n",
      "train_loss = 2.0151, val_loss = 1.8659\n",
      "Epoch 39\n",
      "train_loss = 1.5654, val_loss = 1.8474\n",
      "Epoch 40\n",
      "train_loss = 1.6155, val_loss = 1.9481\n",
      "Epoch 41\n",
      "train_loss = 1.7124, val_loss = 1.8090\n",
      "Epoch 42\n",
      "train_loss = 1.6405, val_loss = 1.7713\n",
      "Epoch 43\n",
      "train_loss = 1.7078, val_loss = 1.7422\n",
      "Epoch 44\n",
      "train_loss = 1.6288, val_loss = 1.8332\n",
      "Epoch 45\n",
      "train_loss = 1.6707, val_loss = 1.9170\n",
      "Epoch 46\n",
      "train_loss = 1.8525, val_loss = 1.8593\n",
      "Epoch 47\n",
      "train_loss = 1.5507, val_loss = 1.8168\n",
      "Epoch 48\n",
      "train_loss = 1.5653, val_loss = 1.8374\n",
      "Epoch 49\n",
      "train_loss = 1.7836, val_loss = 1.7810\n",
      "Epoch 50\n",
      "train_loss = 1.5911, val_loss = 1.7915\n",
      "Epoch 51\n",
      "train_loss = 1.9318, val_loss = 1.8088\n",
      "Epoch 52\n",
      "train_loss = 1.5679, val_loss = 1.8018\n",
      "Epoch 53\n",
      "train_loss = 1.5941, val_loss = 1.8272\n",
      "Epoch 54\n",
      "train_loss = 1.7527, val_loss = 1.7783\n",
      "Epoch 55\n",
      "train_loss = 1.7343, val_loss = 1.7394\n",
      "Epoch 56\n",
      "train_loss = 1.4004, val_loss = 1.8181\n",
      "Epoch 57\n",
      "train_loss = 1.3781, val_loss = 1.8062\n",
      "Epoch 58\n",
      "train_loss = 1.6281, val_loss = 1.8529\n",
      "Epoch 59\n",
      "train_loss = 1.7600, val_loss = 1.9382\n",
      "Epoch 60\n",
      "train_loss = 1.7177, val_loss = 1.7530\n",
      "Epoch 61\n",
      "train_loss = 1.6174, val_loss = 1.7327\n",
      "Epoch 62\n",
      "train_loss = 1.7602, val_loss = 1.8540\n",
      "Epoch 63\n",
      "train_loss = 1.7189, val_loss = 1.8224\n",
      "Epoch 64\n",
      "train_loss = 1.5349, val_loss = 1.8708\n",
      "Epoch 65\n",
      "train_loss = 1.4565, val_loss = 1.8353\n",
      "Epoch 66\n",
      "train_loss = 1.7002, val_loss = 1.8026\n",
      "Epoch 67\n",
      "train_loss = 1.5059, val_loss = 1.8125\n",
      "Epoch 68\n",
      "train_loss = 1.8393, val_loss = 1.8366\n",
      "Epoch 69\n",
      "train_loss = 1.6781, val_loss = 1.7619\n",
      "Epoch 70\n",
      "train_loss = 1.3836, val_loss = 1.7180\n",
      "Epoch 71\n",
      "train_loss = 1.6317, val_loss = 1.7515\n",
      "Epoch 72\n",
      "train_loss = 1.7217, val_loss = 1.7304\n",
      "Epoch 73\n",
      "train_loss = 1.7836, val_loss = 1.8013\n",
      "Epoch 74\n",
      "train_loss = 1.9825, val_loss = 1.8529\n",
      "Epoch 75\n",
      "train_loss = 1.6905, val_loss = 1.7476\n",
      "Epoch 76\n",
      "train_loss = 1.5458, val_loss = 1.8296\n",
      "Epoch 77\n",
      "train_loss = 1.8907, val_loss = 1.7536\n",
      "Epoch 78\n",
      "train_loss = 1.8463, val_loss = 1.7279\n",
      "Epoch 79\n",
      "train_loss = 1.9124, val_loss = 1.7426\n",
      "Epoch 80\n",
      "train_loss = 1.5126, val_loss = 1.7351\n",
      "Epoch 81\n",
      "train_loss = 1.7613, val_loss = 1.8025\n",
      "Epoch 82\n",
      "train_loss = 1.8915, val_loss = 1.8111\n",
      "Epoch 83\n",
      "train_loss = 1.7130, val_loss = 1.8131\n",
      "Epoch 84\n",
      "train_loss = 1.6201, val_loss = 1.7378\n",
      "Epoch 85\n",
      "train_loss = 1.5773, val_loss = 1.7617\n",
      "Epoch 86\n",
      "train_loss = 1.6522, val_loss = 1.8043\n",
      "Epoch 87\n",
      "train_loss = 1.6887, val_loss = 1.7665\n",
      "Epoch 88\n",
      "train_loss = 1.8682, val_loss = 1.7880\n",
      "Epoch 89\n",
      "train_loss = 1.6773, val_loss = 1.7552\n",
      "Epoch 90\n",
      "train_loss = 1.5579, val_loss = 1.7317\n",
      "Epoch 91\n",
      "train_loss = 2.1096, val_loss = 1.8168\n",
      "Epoch 92\n",
      "train_loss = 1.5482, val_loss = 1.7490\n",
      "Epoch 93\n",
      "train_loss = 1.5697, val_loss = 1.8820\n",
      "Epoch 94\n",
      "train_loss = 1.4998, val_loss = 1.7377\n",
      "Epoch 95\n",
      "train_loss = 1.5879, val_loss = 1.7963\n",
      "Epoch 96\n",
      "train_loss = 1.7372, val_loss = 1.7459\n",
      "Epoch 97\n",
      "train_loss = 2.2851, val_loss = 1.8571\n",
      "Epoch 98\n",
      "train_loss = 1.5556, val_loss = 1.9239\n",
      "Epoch 99\n",
      "train_loss = 1.7560, val_loss = 1.7289\n",
      "Epoch 100\n",
      "train_loss = 1.6871, val_loss = 1.9021\n",
      "Epoch 101\n",
      "train_loss = 1.9983, val_loss = 1.7708\n",
      "Epoch 102\n",
      "train_loss = 1.5199, val_loss = 1.7522\n",
      "Epoch 103\n",
      "train_loss = 1.6325, val_loss = 1.8487\n",
      "Epoch 104\n",
      "train_loss = 2.0103, val_loss = 1.8442\n",
      "Epoch 105\n",
      "train_loss = 1.5732, val_loss = 1.7224\n",
      "Epoch 106\n",
      "train_loss = 1.6326, val_loss = 1.7107\n",
      "Epoch 107\n",
      "train_loss = 1.5878, val_loss = 1.8128\n",
      "Epoch 108\n",
      "train_loss = 1.8978, val_loss = 1.8054\n",
      "Epoch 109\n",
      "train_loss = 1.4823, val_loss = 1.8841\n",
      "Epoch 110\n",
      "train_loss = 1.8242, val_loss = 1.7214\n",
      "Epoch 111\n",
      "train_loss = 1.4089, val_loss = 1.7212\n",
      "Epoch 112\n",
      "train_loss = 1.5589, val_loss = 1.8608\n",
      "Epoch 113\n",
      "train_loss = 1.8590, val_loss = 1.8027\n",
      "Epoch 114\n",
      "train_loss = 1.5789, val_loss = 1.7944\n",
      "Epoch 115\n",
      "train_loss = 1.4995, val_loss = 1.7171\n",
      "Epoch 116\n",
      "train_loss = 1.6925, val_loss = 1.7508\n",
      "Epoch 117\n",
      "train_loss = 1.9762, val_loss = 1.9872\n",
      "Epoch 118\n",
      "train_loss = 1.6813, val_loss = 1.8130\n",
      "Epoch 119\n",
      "train_loss = 1.6844, val_loss = 1.7428\n",
      "Epoch 120\n",
      "train_loss = 1.4816, val_loss = 1.7658\n",
      "Epoch 121\n",
      "train_loss = 1.6870, val_loss = 1.8680\n",
      "Epoch 122\n",
      "train_loss = 1.6225, val_loss = 1.7519\n",
      "Epoch 123\n",
      "train_loss = 1.8947, val_loss = 1.7260\n",
      "Epoch 124\n",
      "train_loss = 1.8262, val_loss = 1.7659\n",
      "Epoch 125\n",
      "train_loss = 1.4888, val_loss = 1.7645\n",
      "Epoch 126\n",
      "train_loss = 1.6525, val_loss = 1.7496\n",
      "Epoch 127\n",
      "train_loss = 1.7868, val_loss = 1.7054\n",
      "Epoch 128\n",
      "train_loss = 1.6688, val_loss = 1.7260\n",
      "Epoch 129\n",
      "train_loss = 1.4703, val_loss = 1.7303\n",
      "Epoch 130\n",
      "train_loss = 1.4687, val_loss = 1.8856\n",
      "Epoch 131\n",
      "train_loss = 1.5861, val_loss = 1.7634\n",
      "Epoch 132\n",
      "train_loss = 1.6839, val_loss = 1.7481\n",
      "Epoch 133\n",
      "train_loss = 1.7891, val_loss = 1.7184\n",
      "Epoch 134\n",
      "train_loss = 1.6469, val_loss = 1.7392\n",
      "Epoch 135\n",
      "train_loss = 1.8535, val_loss = 1.7984\n",
      "Epoch 136\n",
      "train_loss = 1.6467, val_loss = 1.7222\n",
      "Epoch 137\n",
      "train_loss = 1.5118, val_loss = 1.7580\n",
      "Epoch 138\n",
      "train_loss = 1.3920, val_loss = 1.9052\n",
      "Epoch 139\n",
      "train_loss = 1.8521, val_loss = 1.8188\n",
      "Epoch 140\n",
      "train_loss = 1.8753, val_loss = 1.7521\n",
      "Epoch 141\n",
      "train_loss = 1.7134, val_loss = 1.7700\n",
      "Epoch 142\n",
      "train_loss = 1.7609, val_loss = 1.7306\n",
      "Epoch 143\n",
      "train_loss = 1.5974, val_loss = 1.8382\n",
      "Epoch 144\n",
      "train_loss = 1.6054, val_loss = 1.7663\n",
      "Epoch 145\n",
      "train_loss = 1.7614, val_loss = 1.7885\n",
      "Epoch 146\n",
      "train_loss = 2.1933, val_loss = 2.0918\n",
      "Epoch 147\n",
      "train_loss = 1.7194, val_loss = 1.7487\n",
      "Epoch 148\n",
      "train_loss = 1.8315, val_loss = 1.7448\n",
      "Epoch 149\n",
      "train_loss = 1.7157, val_loss = 1.7452\n",
      "Epoch 150\n",
      "train_loss = 1.5449, val_loss = 1.7623\n",
      "Epoch 151\n",
      "train_loss = 1.8102, val_loss = 1.6982\n",
      "Epoch 152\n",
      "train_loss = 1.6155, val_loss = 1.7505\n",
      "Epoch 153\n",
      "train_loss = 1.6424, val_loss = 1.7466\n",
      "Epoch 154\n",
      "train_loss = 1.8247, val_loss = 1.7663\n",
      "Epoch 155\n",
      "train_loss = 1.7214, val_loss = 1.7027\n",
      "Epoch 156\n",
      "train_loss = 1.5832, val_loss = 1.7412\n",
      "Epoch 157\n",
      "train_loss = 1.6824, val_loss = 1.7799\n",
      "Epoch 158\n",
      "train_loss = 1.6674, val_loss = 1.8633\n",
      "Epoch 159\n",
      "train_loss = 1.5379, val_loss = 1.7279\n",
      "Epoch 160\n",
      "train_loss = 1.4348, val_loss = 1.7861\n",
      "Epoch 161\n",
      "train_loss = 1.8689, val_loss = 1.8321\n",
      "Epoch 162\n",
      "train_loss = 1.6284, val_loss = 1.7153\n",
      "Epoch 163\n",
      "train_loss = 1.6590, val_loss = 1.7700\n",
      "Epoch 164\n",
      "train_loss = 1.6299, val_loss = 1.7222\n",
      "Epoch 165\n",
      "train_loss = 1.6792, val_loss = 1.7230\n",
      "Epoch 166\n",
      "train_loss = 1.4081, val_loss = 1.7560\n",
      "Epoch 167\n",
      "train_loss = 1.4010, val_loss = 1.7306\n",
      "Epoch 168\n",
      "train_loss = 1.5926, val_loss = 1.7238\n",
      "Epoch 169\n",
      "train_loss = 1.5742, val_loss = 1.7515\n",
      "Epoch 170\n",
      "train_loss = 1.6694, val_loss = 1.7710\n",
      "Epoch 171\n",
      "train_loss = 1.5970, val_loss = 1.7292\n",
      "Epoch 172\n",
      "train_loss = 1.5825, val_loss = 1.7956\n",
      "Epoch 173\n",
      "train_loss = 1.6958, val_loss = 1.7245\n",
      "Epoch 174\n",
      "train_loss = 1.4640, val_loss = 1.7516\n",
      "Epoch 175\n",
      "train_loss = 1.9209, val_loss = 1.8807\n",
      "Epoch 176\n",
      "train_loss = 1.5878, val_loss = 1.7882\n",
      "Epoch 177\n",
      "train_loss = 1.6336, val_loss = 1.7594\n",
      "Epoch 178\n",
      "train_loss = 1.6746, val_loss = 1.7519\n",
      "Epoch 179\n",
      "train_loss = 1.4374, val_loss = 1.7541\n",
      "Epoch 180\n",
      "train_loss = 1.7050, val_loss = 1.8547\n",
      "Epoch 181\n",
      "train_loss = 1.7661, val_loss = 1.7788\n",
      "Epoch 182\n",
      "train_loss = 1.6977, val_loss = 1.7477\n",
      "Epoch 183\n",
      "train_loss = 1.6406, val_loss = 1.7118\n",
      "Epoch 184\n",
      "train_loss = 1.7685, val_loss = 1.8190\n",
      "Epoch 185\n",
      "train_loss = 1.4844, val_loss = 1.7083\n",
      "Epoch 186\n",
      "train_loss = 1.4314, val_loss = 1.7623\n",
      "Epoch 187\n",
      "train_loss = 1.5441, val_loss = 1.7136\n",
      "Epoch 188\n",
      "train_loss = 1.6558, val_loss = 1.7142\n",
      "Epoch 189\n",
      "train_loss = 1.6290, val_loss = 1.7095\n",
      "Epoch 190\n",
      "train_loss = 1.6124, val_loss = 1.8300\n",
      "Epoch 191\n",
      "train_loss = 1.4976, val_loss = 1.7152\n",
      "Epoch 192\n",
      "train_loss = 1.4593, val_loss = 1.7649\n",
      "Epoch 193\n",
      "train_loss = 1.5176, val_loss = 1.7924\n",
      "Epoch 194\n",
      "train_loss = 1.8191, val_loss = 1.7888\n",
      "Epoch 195\n",
      "train_loss = 1.1382, val_loss = 1.9072\n",
      "Epoch 196\n",
      "train_loss = 1.7189, val_loss = 1.7113\n",
      "Epoch 197\n",
      "train_loss = 1.8180, val_loss = 1.7203\n",
      "Epoch 198\n",
      "train_loss = 1.6249, val_loss = 1.7896\n",
      "Epoch 199\n",
      "train_loss = 1.3954, val_loss = 1.7756\n",
      "Epoch 200\n",
      "train_loss = 1.6768, val_loss = 1.7464\n",
      "Epoch 201\n",
      "train_loss = 1.7455, val_loss = 1.7196\n"
     ]
    }
   ],
   "source": [
    "mlp = NeuralNetwork(max_toks, [], NUM_CLASSES, device=\"cuda\")\n",
    "training_loop(mlp, train_loader, val_loader, epochs=EPOCHS, learning_rate=LEARNING_RATE, path=\"models/slpseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.6553\n",
      "accuracy = 0.3445\n",
      "f1 = 0.2521\n",
      "[[0.32790698 0.28116883 0.11111111 0.23076923 0.42857143 0.        ]\n",
      " [0.31162791 0.35519481 0.66666667 0.38461538 0.28571429 1.        ]\n",
      " [0.06744186 0.08441558 0.         0.         0.         0.        ]\n",
      " [0.13255814 0.13961039 0.11111111 0.07692308 0.14285714 0.        ]\n",
      " [0.1255814  0.10779221 0.         0.30769231 0.         0.        ]\n",
      " [0.03488372 0.03181818 0.11111111 0.         0.14285714 0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAGkCAYAAAAIduO+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAUrklEQVR4nO3df2zVhb3/8Xd/2IPTtoIi0lFQ59SAASMKaZybU6Yhhuj+MoRkDTNLtpRFQkyW/vEd+sdS7j9GMwkj+8X9YwS3JWhirjLGBmSZTCxpgpp5xeti+SIwl+/a0t0dsT3fP+6193aC7gDv86E9j0dysvXsc/i8PhF57vxoaahUKpUAgCSNRQ8AYHoTGgBSCQ0AqYQGgFRCA0AqoQEgldAAkEpoAEglNACkEhoAUtVNaDZv3hzXXnttzJgxI5YvXx6vvPJK0ZPS7d+/P1atWhUdHR3R0NAQzz33XNGT0vX19cUdd9wRra2tcfXVV8dDDz0Ub775ZtGz0m3ZsiUWL14cbW1t0dbWFl1dXfHiiy8WPavmNm3aFA0NDbF+/fqip6R6/PHHo6GhYdLt5ptvLnrWWdVFaJ599tnYsGFDbNy4MQ4dOhRLliyJ+++/P06ePFn0tFSjo6OxZMmS2Lx5c9FTambfvn3R09MTBw4ciN27d8fp06fjvvvui9HR0aKnpZo3b15s2rQp+vv749VXX4177rknHnzwwXj99deLnlYzBw8ejK1bt8bixYuLnlITixYtivfee2/i9rvf/a7oSWdXqQPLli2r9PT0THw9NjZW6ejoqPT19RW4qrYiorJz586iZ9TcyZMnKxFR2bdvX9FTam7mzJmVH/3oR0XPqImRkZHK5z//+cru3bsrX/rSlyqPPvpo0ZNSbdy4sbJkyZKiZ/zTpv0zmg8++CD6+/tjxYoVE/c1NjbGihUr4uWXXy5wGbUwNDQUERGzZs0qeEntjI2NxY4dO2J0dDS6urqKnlMTPT098cADD0z693y6e+utt6KjoyOuv/76WLNmTbz77rtFTzqr5qIHZHv//fdjbGws5syZM+n+OXPmxB//+MeCVlEL4+PjsX79+rjzzjvjlltuKXpOusOHD0dXV1f8/e9/j8svvzx27twZCxcuLHpWuh07dsShQ4fi4MGDRU+pmeXLl8e2bdvipptuivfeey+eeOKJuOuuu+K1116L1tbWoud9zLQPDfWrp6cnXnvttYv7tesL6KabboqBgYEYGhqKX/7yl9Hd3R379u2b1rEZHByMRx99NHbv3h0zZswoek7NrFy5cuK/L168OJYvXx4LFiyIn//85/HII48UuOzMpn1orrrqqmhqaooTJ05Muv/EiRNxzTXXFLSKbOvWrYsXXngh9u/fH/PmzSt6Tk20tLTEDTfcEBERS5cujYMHD8bTTz8dW7duLXhZnv7+/jh58mTcdtttE/eNjY3F/v3745lnnolyuRxNTU0FLqyNK664Im688cY4cuRI0VPOaNq/R9PS0hJLly6NPXv2TNw3Pj4ee/bsqZvXr+tJpVKJdevWxc6dO+M3v/lNXHfddUVPKsz4+HiUy+WiZ6S699574/DhwzEwMDBxu/3222PNmjUxMDBQF5GJiDh16lS8/fbbMXfu3KKnnNG0f0YTEbFhw4bo7u6O22+/PZYtWxZPPfVUjI6Oxtq1a4uelurUqVOT/h/OO++8EwMDAzFr1qyYP39+gcvy9PT0xPbt2+P555+P1tbWOH78eEREtLe3x6WXXlrwujy9vb2xcuXKmD9/foyMjMT27dtj7969sWvXrqKnpWptbf3Y+2+XXXZZXHnlldP6fbnHHnssVq1aFQsWLIhjx47Fxo0bo6mpKVavXl30tDMr+mNvtfL973+/Mn/+/EpLS0tl2bJllQMHDhQ9Kd1vf/vbSkR87Nbd3V30tDRnut6IqPz0pz8telqqr3/965UFCxZUWlpaKrNnz67ce++9lV/96ldFzypEPXy8+eGHH67MnTu30tLSUvnsZz9befjhhytHjhwpetZZNVQqlUpBjQOgDkz792gAKJbQAJBKaABIJTQApBIaAFIJDQCp6io05XI5Hn/88Wn/3dL/yHW77nrgui/e666r76MZHh6O9vb2GBoaira2tqLn1Izrdt31wHVfvNddV89oAKg9oQEgVc1/qOb4+HgcO3YsWltbo6GhoabnHh4envSf9cJ1u+564Lprf92VSiVGRkaio6MjGhvP/ryl5u/RHD16NDo7O2t5SgASDQ4OfuLf+1TzZzQf/TWji1b/n2hqqZ+/ES8i4pLRuvncxSRXDLxf9IRC/O1zM4ueUIg/33pJ0RMKMe9f/lD0hJr7ME7H7+LfPvWvj655aD56uaypZUbdhab5g/oMTXNTqegJhWi+pL5+f3+kqVSfoWluqMPr/u8/0j7tbRAfBgAgldAAkEpoAEglNACkEhoAUgkNAKmEBoBUQgNAKqEBIJXQAJBKaABIJTQApBIaAFIJDQCphAaAVEIDQCqhASCV0ACQSmgASCU0AKQSGgBSCQ0AqYQGgFRCA0CqcwrN5s2b49prr40ZM2bE8uXL45VXXrnQuwCYJqoOzbPPPhsbNmyIjRs3xqFDh2LJkiVx//33x8mTJzP2ATDFVR2aJ598Mr7xjW/E2rVrY+HChfGDH/wgPvOZz8RPfvKTjH0ATHFVheaDDz6I/v7+WLFixf/8Ao2NsWLFinj55ZfP+JhyuRzDw8OTbgDUj6pC8/7778fY2FjMmTNn0v1z5syJ48ePn/ExfX190d7ePnHr7Ow897UATDnpnzrr7e2NoaGhidvg4GD2KQG4iDRXc/BVV10VTU1NceLEiUn3nzhxIq655pozPqZUKkWpVDr3hQBMaVU9o2lpaYmlS5fGnj17Ju4bHx+PPXv2RFdX1wUfB8DUV9UzmoiIDRs2RHd3d9x+++2xbNmyeOqpp2J0dDTWrl2bsQ+AKa7q0Dz88MPx5z//Ob773e/G8ePH49Zbb42XXnrpYx8QAICIcwhNRMS6deti3bp1F3oLANOQn3UGQCqhASCV0ACQSmgASCU0AKQSGgBSCQ0AqYQGgFRCA0AqoQEgldAAkEpoAEglNACkEhoAUgkNAKmEBoBUQgNAKqEBIJXQAJBKaABIJTQApBIaAFIJDQCphAaAVM1Fnbjxw4jGOsvcf15ZZxf838afaih6QiH+tueSoicUov3t8aInFGLXsYGiJ9Tc8Mh4zLzx04+rzz/5AKgZoQEgldAAkEpoAEglNACkEhoAUgkNAKmEBoBUQgNAKqEBIJXQAJBKaABIJTQApBIaAFIJDQCphAaAVEIDQCqhASCV0ACQSmgASCU0AKQSGgBSCQ0AqYQGgFRCA0AqoQEgVdWh2b9/f6xatSo6OjqioaEhnnvuuYRZAEwXVYdmdHQ0lixZEps3b87YA8A001ztA1auXBkrV67M2ALANFR1aKpVLpejXC5PfD08PJx9SgAuIukfBujr64v29vaJW2dnZ/YpAbiIpIemt7c3hoaGJm6Dg4PZpwTgIpL+0lmpVIpSqZR9GgAuUr6PBoBUVT+jOXXqVBw5cmTi63feeScGBgZi1qxZMX/+/As6DoCpr+rQvPrqq/HlL3954usNGzZERER3d3ds27btgg0DYHqoOjR33313VCqVjC0ATEPeowEgldAAkEpoAEglNACkEhoAUgkNAKmEBoBUQgNAKqEBIJXQAJBKaABIJTQApBIaAFIJDQCphAaAVEIDQCqhASCV0ACQSmgASCU0AKQSGgBSCQ0AqYQGgFRCA0Cq5qJOfNWv/xTNjS1Fnb4Yl1xS9IJCfLj5aNETCtEeR4qeQA3dv/3WoifU3IeV0xHxH596nGc0AKQSGgBSCQ0AqYQGgFRCA0AqoQEgldAAkEpoAEglNACkEhoAUgkNAKmEBoBUQgNAKqEBIJXQAJBKaABIJTQApBIaAFIJDQCphAaAVEIDQCqhASCV0ACQSmgASCU0AKQSGgBSVRWavr6+uOOOO6K1tTWuvvrqeOihh+LNN9/M2gbANFBVaPbt2xc9PT1x4MCB2L17d5w+fTruu+++GB0dzdoHwBTXXM3BL7300qSvt23bFldffXX09/fHF7/4xQs6DIDpoarQ/KOhoaGIiJg1a9ZZjymXy1Eulye+Hh4ePp9TAjDFnPOHAcbHx2P9+vVx5513xi233HLW4/r6+qK9vX3i1tnZea6nBGAKOufQ9PT0xGuvvRY7duz4xON6e3tjaGho4jY4OHiupwRgCjqnl87WrVsXL7zwQuzfvz/mzZv3iceWSqUolUrnNA6Aqa+q0FQqlfj2t78dO3fujL1798Z1112XtQuAaaKq0PT09MT27dvj+eefj9bW1jh+/HhERLS3t8ell16aMhCAqa2q92i2bNkSQ0NDcffdd8fcuXMnbs8++2zWPgCmuKpfOgOAavhZZwCkEhoAUgkNAKmEBoBUQgNAKqEBIJXQAJBKaABIJTQApBIaAFIJDQCphAaAVEIDQCqhASCV0ACQSmgASCU0AKQSGgBSCQ0AqYQGgFRCA0AqoQEgldAAkEpoAEjVXNSJx6+8IsabSkWdvhhNDUUvKETTpZ8rekIhGkb/s+gJhRiffUXREwoxPvBG0RMuWp7RAJBKaABIJTQApBIaAFIJDQCphAaAVEIDQCqhASCV0ACQSmgASCU0AKQSGgBSCQ0AqYQGgFRCA0AqoQEgldAAkEpoAEglNACkEhoAUgkNAKmEBoBUQgNAKqEBIJXQAJBKaABIVVVotmzZEosXL462trZoa2uLrq6uePHFF7O2ATANVBWaefPmxaZNm6K/vz9effXVuOeee+LBBx+M119/PWsfAFNcczUHr1q1atLX3/ve92LLli1x4MCBWLRo0QUdBsD0UFVo/rexsbH4xS9+EaOjo9HV1XXW48rlcpTL5Ymvh4eHz/WUAExBVX8Y4PDhw3H55ZdHqVSKb37zm7Fz585YuHDhWY/v6+uL9vb2iVtnZ+d5DQZgaqk6NDfddFMMDAzEH/7wh/jWt74V3d3d8cYbb5z1+N7e3hgaGpq4DQ4OntdgAKaWql86a2lpiRtuuCEiIpYuXRoHDx6Mp59+OrZu3XrG40ulUpRKpfNbCcCUdd7fRzM+Pj7pPRgA+N+qekbT29sbK1eujPnz58fIyEhs37499u7dG7t27craB8AUV1VoTp48GV/72tfivffei/b29li8eHHs2rUrvvKVr2TtA2CKqyo0P/7xj7N2ADBN+VlnAKQSGgBSCQ0AqYQGgFRCA0AqoQEgldAAkEpoAEglNACkEhoAUgkNAKmEBoBUQgNAKqEBIJXQAJBKaABIJTQApBIaAFIJDQCphAaAVEIDQCqhASCV0ACQSmgASNVc1IkbKpVoqFSKOn0x/vZB0QsKMfbWfxQ9oRD/r7ur6AmFmPmvLxc9gYuMZzQApBIaAFIJDQCphAaAVEIDQCqhASCV0ACQSmgASCU0AKQSGgBSCQ0AqYQGgFRCA0AqoQEgldAAkEpoAEglNACkEhoAUgkNAKmEBoBUQgNAKqEBIJXQAJBKaABIJTQApBIaAFKdV2g2bdoUDQ0NsX79+gs0B4Dp5pxDc/Dgwdi6dWssXrz4Qu4BYJo5p9CcOnUq1qxZEz/84Q9j5syZF3oTANPIOYWmp6cnHnjggVixYsWnHlsul2N4eHjSDYD60VztA3bs2BGHDh2KgwcP/lPH9/X1xRNPPFH1MACmh6qe0QwODsajjz4aP/vZz2LGjBn/1GN6e3tjaGho4jY4OHhOQwGYmqp6RtPf3x8nT56M2267beK+sbGx2L9/fzzzzDNRLpejqalp0mNKpVKUSqULsxaAKaeq0Nx7771x+PDhSfetXbs2br755vjOd77zscgAQFWhaW1tjVtuuWXSfZdddllceeWVH7sfACL8ZAAAklX9qbN/tHfv3gswA4DpyjMaAFIJDQCphAaAVEIDQCqhASCV0ACQSmgASCU0AKQSGgBSCQ0AqYQGgFRCA0AqoQEgldAAkEpoAEglNACkEhoAUgkNAKmEBoBUQgNAKqEBIJXQAJBKaABIJTQApGou6sSVd/9vVBpaijo9NdR04+eKnlCImf/6ctETCtF468KiJxRifOCNoidctDyjASCV0ACQSmgASCU0AKQSGgBSCQ0AqYQGgFRCA0AqoQEgldAAkEpoAEglNACkEhoAUgkNAKmEBoBUQgNAKqEBIJXQAJBKaABIJTQApBIaAFIJDQCphAaAVEIDQCqhASCV0ACQqqrQPP7449HQ0DDpdvPNN2dtA2AaaK72AYsWLYpf//rX//MLNFf9SwBQR6quRHNzc1xzzTUZWwCYhqp+j+att96Kjo6OuP7662PNmjXx7rvvfuLx5XI5hoeHJ90AqB9VhWb58uWxbdu2eOmll2LLli3xzjvvxF133RUjIyNnfUxfX1+0t7dP3Do7O897NABTR0OlUqmc64P/+te/xoIFC+LJJ5+MRx555IzHlMvlKJfLE18PDw9HZ2dn3HPZ6mhuaDnXUzOFNHy2Pl9qHfv3t4ueUIjGWxcWPaEQ4wNvFD2h5j6snI698XwMDQ1FW1vbWY87r3fyr7jiirjxxhvjyJEjZz2mVCpFqVQ6n9MAMIWd1/fRnDp1Kt5+++2YO3fuhdoDwDRTVWgee+yx2LdvX/zpT3+K3//+9/HVr341mpqaYvXq1Vn7AJjiqnrp7OjRo7F69er4y1/+ErNnz44vfOELceDAgZg9e3bWPgCmuKpCs2PHjqwdAExTftYZAKmEBoBUQgNAKqEBIJXQAJBKaABIJTQApBIaAFIJDQCphAaAVEIDQCqhASCV0ACQSmgASCU0AKQSGgBSCQ0AqYQGgFRCA0AqoQEgldAAkEpoAEglNACkaq71CSuVSkREfFg5XetTU5CGsXLREwoxVqe/xxvr9J/3eB3+8/4w/uuaP/pz/WwaKp92xAV29OjR6OzsrOUpAUg0ODgY8+bNO+v/XvPQjI+Px7Fjx6K1tTUaGhpqeeoYHh6Ozs7OGBwcjLa2tpqeu0iu23XXA9dd++uuVCoxMjISHR0d0dh49ndiav7SWWNj4yeWrxba2trq6jfiR1x3fXHd9aWo625vb//UY3wYAIBUQgNAqroKTalUio0bN0apVCp6Sk25btddD1z3xXvdNf8wAAD1pa6e0QBQe0IDQCqhASCV0ACQSmgASCU0AKQSGgBSCQ0Aqf4/w+cNU6hmK+UAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, _, _, cmat = evaluate(model=mlp, val_dl=test_loader)\n",
    "print(cmat)\n",
    "plt.matshow(cmat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because an MLP operates on fixed size inputs, we will use the entire fixed size input for this\n",
    "train_data = CharTokenDataset(X_train, Y_train, tokenizer=None, max_seq_length=max_toks)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_data = CharTokenDataset(X_val, Y_val, tokenizer=None, max_seq_length=max_toks)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Batch size is hard coded to 1 for accuracy purposes. \n",
    "test_data = CharTokenDataset(X_test, Y_test, tokenizer=None, max_seq_length=max_toks)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "train_loss = 1.7771, val_loss = 1.6591\n",
      "Epoch 2\n",
      "train_loss = 1.6317, val_loss = 1.6356\n",
      "Epoch 3\n",
      "train_loss = 1.7473, val_loss = 1.6129\n",
      "Epoch 4\n",
      "train_loss = 1.5172, val_loss = 1.6177\n",
      "Epoch 5\n",
      "train_loss = 1.6969, val_loss = 1.6159\n",
      "Epoch 6\n",
      "train_loss = 1.8144, val_loss = 1.6107\n",
      "Epoch 7\n",
      "train_loss = 1.6213, val_loss = 1.5982\n",
      "Epoch 8\n",
      "train_loss = 1.7224, val_loss = 1.5936\n",
      "Epoch 9\n",
      "train_loss = 1.6084, val_loss = 1.5857\n",
      "Epoch 10\n",
      "train_loss = 1.4453, val_loss = 1.5855\n",
      "Epoch 11\n",
      "train_loss = 1.4773, val_loss = 1.5812\n",
      "Epoch 12\n",
      "train_loss = 1.6663, val_loss = 1.5823\n",
      "Epoch 13\n",
      "train_loss = 1.5502, val_loss = 1.5807\n",
      "Epoch 14\n",
      "train_loss = 1.5330, val_loss = 1.5847\n",
      "Epoch 15\n",
      "train_loss = 1.6405, val_loss = 1.5829\n",
      "Epoch 16\n",
      "train_loss = 1.4082, val_loss = 1.5831\n",
      "Epoch 17\n",
      "train_loss = 1.6473, val_loss = 1.5820\n",
      "Epoch 18\n",
      "train_loss = 1.5996, val_loss = 1.5807\n",
      "Epoch 19\n",
      "train_loss = 1.6273, val_loss = 1.5809\n",
      "Epoch 20\n",
      "train_loss = 1.8354, val_loss = 1.5817\n",
      "Epoch 21\n",
      "train_loss = 1.6578, val_loss = 1.5797\n",
      "Epoch 22\n",
      "train_loss = 1.4926, val_loss = 1.5796\n",
      "Epoch 23\n",
      "train_loss = 1.5391, val_loss = 1.5830\n",
      "Epoch 24\n",
      "train_loss = 1.5061, val_loss = 1.5812\n",
      "Epoch 25\n",
      "train_loss = 1.6384, val_loss = 1.5814\n",
      "Epoch 26\n",
      "train_loss = 1.6114, val_loss = 1.5798\n",
      "Epoch 27\n",
      "train_loss = 1.6473, val_loss = 1.5796\n",
      "Epoch 28\n",
      "train_loss = 1.7043, val_loss = 1.5797\n",
      "Epoch 29\n",
      "train_loss = 1.5017, val_loss = 1.5792\n",
      "Epoch 30\n",
      "train_loss = 1.5853, val_loss = 1.5809\n",
      "Epoch 31\n",
      "train_loss = 1.7743, val_loss = 1.5810\n",
      "Epoch 32\n",
      "train_loss = 1.6940, val_loss = 1.5799\n",
      "Epoch 33\n",
      "train_loss = 1.5962, val_loss = 1.5815\n",
      "Epoch 34\n",
      "train_loss = 1.4598, val_loss = 1.5800\n",
      "Epoch 35\n",
      "train_loss = 1.5833, val_loss = 1.5802\n",
      "Epoch 36\n",
      "train_loss = 1.6352, val_loss = 1.5829\n",
      "Epoch 37\n",
      "train_loss = 1.6292, val_loss = 1.5813\n",
      "Epoch 38\n",
      "train_loss = 1.6014, val_loss = 1.5804\n",
      "Epoch 39\n",
      "train_loss = 1.4930, val_loss = 1.5820\n",
      "Epoch 40\n",
      "train_loss = 1.4782, val_loss = 1.5823\n",
      "Epoch 41\n",
      "train_loss = 1.7585, val_loss = 1.5810\n",
      "Epoch 42\n",
      "train_loss = 1.6580, val_loss = 1.5803\n",
      "Epoch 43\n",
      "train_loss = 1.5544, val_loss = 1.5815\n",
      "Epoch 44\n",
      "train_loss = 1.6004, val_loss = 1.5804\n",
      "Epoch 45\n",
      "train_loss = 1.6286, val_loss = 1.5826\n",
      "Epoch 46\n",
      "train_loss = 1.5710, val_loss = 1.5785\n",
      "Epoch 47\n",
      "train_loss = 1.4145, val_loss = 1.5793\n",
      "Epoch 48\n",
      "train_loss = 1.7138, val_loss = 1.5824\n",
      "Epoch 49\n",
      "train_loss = 1.5820, val_loss = 1.5812\n",
      "Epoch 50\n",
      "train_loss = 1.5647, val_loss = 1.5801\n",
      "Epoch 51\n",
      "train_loss = 1.6992, val_loss = 1.5793\n",
      "Epoch 52\n",
      "train_loss = 1.5950, val_loss = 1.5808\n",
      "Epoch 53\n",
      "train_loss = 1.4528, val_loss = 1.5797\n",
      "Epoch 54\n",
      "train_loss = 1.3336, val_loss = 1.5806\n",
      "Epoch 55\n",
      "train_loss = 1.6458, val_loss = 1.5824\n",
      "Epoch 56\n",
      "train_loss = 1.4795, val_loss = 1.5800\n",
      "Epoch 57\n",
      "train_loss = 1.5099, val_loss = 1.5793\n",
      "Epoch 58\n",
      "train_loss = 1.5925, val_loss = 1.5827\n",
      "Epoch 59\n",
      "train_loss = 1.5454, val_loss = 1.5807\n",
      "Epoch 60\n",
      "train_loss = 1.6837, val_loss = 1.5787\n",
      "Epoch 61\n",
      "train_loss = 1.6136, val_loss = 1.5804\n",
      "Epoch 62\n",
      "train_loss = 1.6888, val_loss = 1.5782\n",
      "Epoch 63\n",
      "train_loss = 1.7401, val_loss = 1.5790\n",
      "Epoch 64\n",
      "train_loss = 1.4828, val_loss = 1.5805\n",
      "Epoch 65\n",
      "train_loss = 1.6555, val_loss = 1.5834\n",
      "Epoch 66\n",
      "train_loss = 1.4459, val_loss = 1.5796\n",
      "Epoch 67\n",
      "train_loss = 1.5749, val_loss = 1.5813\n",
      "Epoch 68\n",
      "train_loss = 1.5597, val_loss = 1.5801\n",
      "Epoch 69\n",
      "train_loss = 1.6221, val_loss = 1.5799\n",
      "Epoch 70\n",
      "train_loss = 1.7423, val_loss = 1.5794\n",
      "Epoch 71\n",
      "train_loss = 1.4053, val_loss = 1.5788\n",
      "Epoch 72\n",
      "train_loss = 1.4858, val_loss = 1.5805\n",
      "Epoch 73\n",
      "train_loss = 1.6927, val_loss = 1.5810\n",
      "Epoch 74\n",
      "train_loss = 1.5508, val_loss = 1.5810\n",
      "Epoch 75\n",
      "train_loss = 1.4821, val_loss = 1.5817\n",
      "Epoch 76\n",
      "train_loss = 1.4129, val_loss = 1.5806\n",
      "Epoch 77\n",
      "train_loss = 1.5524, val_loss = 1.5841\n",
      "Epoch 78\n",
      "train_loss = 1.6025, val_loss = 1.5814\n",
      "Epoch 79\n",
      "train_loss = 1.6241, val_loss = 1.5788\n",
      "Epoch 80\n",
      "train_loss = 1.4311, val_loss = 1.5807\n",
      "Epoch 81\n",
      "train_loss = 1.7000, val_loss = 1.5795\n",
      "Epoch 82\n",
      "train_loss = 1.7623, val_loss = 1.5833\n",
      "Epoch 83\n",
      "train_loss = 1.5965, val_loss = 1.5777\n",
      "Epoch 84\n",
      "train_loss = 1.5850, val_loss = 1.5782\n",
      "Epoch 85\n",
      "train_loss = 1.6302, val_loss = 1.5811\n",
      "Epoch 86\n",
      "train_loss = 1.3657, val_loss = 1.5799\n",
      "Epoch 87\n",
      "train_loss = 1.4792, val_loss = 1.5820\n",
      "Epoch 88\n",
      "train_loss = 1.5329, val_loss = 1.5797\n",
      "Epoch 89\n",
      "train_loss = 1.4679, val_loss = 1.5804\n",
      "Epoch 90\n",
      "train_loss = 1.4958, val_loss = 1.5809\n",
      "Epoch 91\n",
      "train_loss = 1.5719, val_loss = 1.5819\n",
      "Epoch 92\n",
      "train_loss = 1.5152, val_loss = 1.5777\n",
      "Epoch 93\n",
      "train_loss = 1.7578, val_loss = 1.5787\n",
      "Epoch 94\n",
      "train_loss = 1.5205, val_loss = 1.5813\n",
      "Epoch 95\n",
      "train_loss = 1.5871, val_loss = 1.5803\n",
      "Epoch 96\n",
      "train_loss = 1.7054, val_loss = 1.5797\n",
      "Epoch 97\n",
      "train_loss = 1.6011, val_loss = 1.5806\n",
      "Epoch 98\n",
      "train_loss = 1.6977, val_loss = 1.5813\n",
      "Epoch 99\n",
      "train_loss = 1.5185, val_loss = 1.5798\n",
      "Epoch 100\n",
      "train_loss = 1.4356, val_loss = 1.5835\n",
      "Epoch 101\n",
      "train_loss = 1.5064, val_loss = 1.5790\n",
      "Epoch 102\n",
      "train_loss = 1.6789, val_loss = 1.5808\n",
      "Epoch 103\n",
      "train_loss = 1.4798, val_loss = 1.5797\n",
      "Epoch 104\n",
      "train_loss = 1.6142, val_loss = 1.5790\n",
      "Epoch 105\n",
      "train_loss = 1.5928, val_loss = 1.5826\n",
      "Epoch 106\n",
      "train_loss = 1.5056, val_loss = 1.5815\n",
      "Epoch 107\n",
      "train_loss = 1.3801, val_loss = 1.5791\n",
      "Epoch 108\n",
      "train_loss = 1.6443, val_loss = 1.5819\n",
      "Epoch 109\n",
      "train_loss = 1.4442, val_loss = 1.5828\n",
      "Epoch 110\n",
      "train_loss = 1.4687, val_loss = 1.5792\n",
      "Epoch 111\n",
      "train_loss = 1.7605, val_loss = 1.5775\n",
      "Epoch 112\n",
      "train_loss = 1.7588, val_loss = 1.5832\n",
      "Epoch 113\n",
      "train_loss = 1.5392, val_loss = 1.5817\n",
      "Epoch 114\n",
      "train_loss = 1.5406, val_loss = 1.5817\n",
      "Epoch 115\n",
      "train_loss = 1.5334, val_loss = 1.5819\n",
      "Epoch 116\n",
      "train_loss = 1.7382, val_loss = 1.5829\n",
      "Epoch 117\n",
      "train_loss = 1.5775, val_loss = 1.5822\n",
      "Epoch 118\n",
      "train_loss = 1.4638, val_loss = 1.5800\n",
      "Epoch 119\n",
      "train_loss = 1.6202, val_loss = 1.5816\n",
      "Epoch 120\n",
      "train_loss = 1.6064, val_loss = 1.5827\n",
      "Epoch 121\n",
      "train_loss = 1.6648, val_loss = 1.5781\n",
      "Epoch 122\n",
      "train_loss = 1.5476, val_loss = 1.5846\n",
      "Epoch 123\n",
      "train_loss = 1.5573, val_loss = 1.5866\n",
      "Epoch 124\n",
      "train_loss = 1.6703, val_loss = 1.5860\n",
      "Epoch 125\n",
      "train_loss = 1.4551, val_loss = 1.5838\n",
      "Epoch 126\n",
      "train_loss = 1.5478, val_loss = 1.5827\n",
      "Epoch 127\n",
      "train_loss = 1.7235, val_loss = 1.5815\n",
      "Epoch 128\n",
      "train_loss = 1.4817, val_loss = 1.5818\n",
      "Epoch 129\n",
      "train_loss = 1.4782, val_loss = 1.5798\n",
      "Epoch 130\n",
      "train_loss = 1.5775, val_loss = 1.5810\n",
      "Epoch 131\n",
      "train_loss = 1.4650, val_loss = 1.5819\n",
      "Epoch 132\n",
      "train_loss = 1.5585, val_loss = 1.5783\n",
      "Epoch 133\n",
      "train_loss = 1.5647, val_loss = 1.5793\n",
      "Epoch 134\n",
      "train_loss = 1.5254, val_loss = 1.5851\n",
      "Epoch 135\n",
      "train_loss = 1.6489, val_loss = 1.5836\n",
      "Epoch 136\n",
      "train_loss = 1.4981, val_loss = 1.5852\n",
      "Epoch 137\n",
      "train_loss = 1.5047, val_loss = 1.5798\n",
      "Epoch 138\n",
      "train_loss = 1.5055, val_loss = 1.5827\n",
      "Epoch 139\n",
      "train_loss = 1.7710, val_loss = 1.5839\n",
      "Epoch 140\n",
      "train_loss = 1.5417, val_loss = 1.5848\n",
      "Epoch 141\n",
      "train_loss = 1.7174, val_loss = 1.5819\n",
      "Epoch 142\n",
      "train_loss = 1.5207, val_loss = 1.5792\n",
      "Epoch 143\n",
      "train_loss = 1.4186, val_loss = 1.5801\n",
      "Epoch 144\n",
      "train_loss = 1.4957, val_loss = 1.5791\n",
      "Epoch 145\n",
      "train_loss = 1.8050, val_loss = 1.5825\n",
      "Epoch 146\n",
      "train_loss = 1.4090, val_loss = 1.5837\n",
      "Epoch 147\n",
      "train_loss = 1.5483, val_loss = 1.5853\n",
      "Epoch 148\n",
      "train_loss = 1.6571, val_loss = 1.5843\n",
      "Epoch 149\n",
      "train_loss = 1.5195, val_loss = 1.5842\n",
      "Epoch 150\n",
      "train_loss = 1.7193, val_loss = 1.5847\n",
      "Epoch 151\n",
      "train_loss = 1.4928, val_loss = 1.5848\n",
      "Epoch 152\n",
      "train_loss = 1.6117, val_loss = 1.5784\n",
      "Epoch 153\n",
      "train_loss = 1.5432, val_loss = 1.5842\n",
      "Epoch 154\n",
      "train_loss = 1.4947, val_loss = 1.5832\n",
      "Epoch 155\n",
      "train_loss = 1.4587, val_loss = 1.5893\n",
      "Epoch 156\n",
      "train_loss = 1.4852, val_loss = 1.5840\n",
      "Epoch 157\n",
      "train_loss = 1.5266, val_loss = 1.5856\n",
      "Epoch 158\n",
      "train_loss = 1.5955, val_loss = 1.5851\n",
      "Epoch 159\n",
      "train_loss = 1.6292, val_loss = 1.5807\n",
      "Epoch 160\n",
      "train_loss = 1.6392, val_loss = 1.5818\n",
      "Epoch 161\n",
      "train_loss = 1.3960, val_loss = 1.5881\n",
      "Epoch 162\n",
      "train_loss = 1.4568, val_loss = 1.5897\n",
      "Epoch 163\n",
      "train_loss = 1.5496, val_loss = 1.5847\n",
      "Epoch 164\n",
      "train_loss = 1.5721, val_loss = 1.5789\n",
      "Epoch 165\n",
      "train_loss = 1.6089, val_loss = 1.5827\n",
      "Epoch 166\n",
      "train_loss = 1.5376, val_loss = 1.5825\n",
      "Epoch 167\n",
      "train_loss = 1.7186, val_loss = 1.5881\n",
      "Epoch 168\n",
      "train_loss = 1.4815, val_loss = 1.5860\n",
      "Epoch 169\n",
      "train_loss = 1.6615, val_loss = 1.5854\n",
      "Epoch 170\n",
      "train_loss = 1.5599, val_loss = 1.5852\n",
      "Epoch 171\n",
      "train_loss = 1.6191, val_loss = 1.5853\n",
      "Epoch 172\n",
      "train_loss = 1.4232, val_loss = 1.5868\n",
      "Epoch 173\n",
      "train_loss = 1.6664, val_loss = 1.5845\n",
      "Epoch 174\n",
      "train_loss = 1.4999, val_loss = 1.5877\n",
      "Epoch 175\n",
      "train_loss = 1.5175, val_loss = 1.5907\n",
      "Epoch 176\n",
      "train_loss = 1.6044, val_loss = 1.5853\n",
      "Epoch 177\n",
      "train_loss = 1.4624, val_loss = 1.5844\n",
      "Epoch 178\n",
      "train_loss = 1.5223, val_loss = 1.5863\n",
      "Epoch 179\n",
      "train_loss = 1.6669, val_loss = 1.5907\n",
      "Epoch 180\n",
      "train_loss = 1.5265, val_loss = 1.5882\n",
      "Epoch 181\n",
      "train_loss = 1.6982, val_loss = 1.5850\n",
      "Epoch 182\n",
      "train_loss = 1.5344, val_loss = 1.5878\n",
      "Epoch 183\n",
      "train_loss = 1.5964, val_loss = 1.5895\n",
      "Epoch 184\n",
      "train_loss = 1.6854, val_loss = 1.5866\n",
      "Epoch 185\n",
      "train_loss = 1.4908, val_loss = 1.5947\n",
      "Epoch 186\n",
      "train_loss = 1.7217, val_loss = 1.5855\n",
      "Epoch 187\n",
      "train_loss = 1.5094, val_loss = 1.5839\n",
      "Epoch 188\n",
      "train_loss = 1.4540, val_loss = 1.5867\n",
      "Epoch 189\n",
      "train_loss = 1.3813, val_loss = 1.5881\n",
      "Epoch 190\n",
      "train_loss = 1.6206, val_loss = 1.5901\n",
      "Epoch 191\n",
      "train_loss = 1.6822, val_loss = 1.5873\n",
      "Epoch 192\n",
      "train_loss = 1.5175, val_loss = 1.5850\n",
      "Epoch 193\n",
      "train_loss = 1.6253, val_loss = 1.5858\n",
      "Epoch 194\n",
      "train_loss = 1.6752, val_loss = 1.5901\n",
      "Epoch 195\n",
      "train_loss = 1.6101, val_loss = 1.5855\n",
      "Epoch 196\n",
      "train_loss = 1.3145, val_loss = 1.5866\n",
      "Epoch 197\n",
      "train_loss = 1.3644, val_loss = 1.5860\n",
      "Epoch 198\n",
      "train_loss = 1.8137, val_loss = 1.5891\n",
      "Epoch 199\n",
      "train_loss = 1.4755, val_loss = 1.5866\n",
      "Epoch 200\n",
      "train_loss = 1.6712, val_loss = 1.5884\n",
      "Epoch 201\n",
      "train_loss = 1.4398, val_loss = 1.5926\n"
     ]
    }
   ],
   "source": [
    "mlp = NeuralNetwork(max_toks, [100, 100, 100], NUM_CLASSES, device=\"cuda\")\n",
    "training_loop(mlp, train_loader, val_loader, epochs=EPOCHS, learning_rate=LEARNING_RATE, path=\"models/mlpseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.5655\n",
      "accuracy = 0.3370\n",
      "f1 = 0.2498\n",
      "[[0.29693487 0.28834356 0.         0.33333333 0.2        0.        ]\n",
      " [0.33333333 0.3524199  0.         0.16666667 0.6        0.        ]\n",
      " [0.07279693 0.08248125 0.         0.         0.         0.        ]\n",
      " [0.13601533 0.13769598 0.         0.33333333 0.         0.        ]\n",
      " [0.13409962 0.10497614 0.         0.         0.         0.        ]\n",
      " [0.02681992 0.03408316 0.         0.16666667 0.2        0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, _, _, cmat = evaluate(model=mlp, val_dl=test_loader)\n",
    "print(cmat)\n",
    "plt.matshow(cmat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "train_loss = 1.8983, val_loss = 1.7939\n",
      "Epoch 2\n",
      "train_loss = 1.8004, val_loss = 1.7927\n",
      "Epoch 3\n",
      "train_loss = 1.8076, val_loss = 1.7933\n",
      "Epoch 4\n",
      "train_loss = 1.8109, val_loss = 1.7920\n",
      "Epoch 5\n",
      "train_loss = 1.7929, val_loss = 1.7919\n",
      "Epoch 6\n",
      "train_loss = 1.8069, val_loss = 1.7935\n",
      "Epoch 7\n",
      "train_loss = 1.8155, val_loss = 1.7924\n",
      "Epoch 8\n",
      "train_loss = 1.7893, val_loss = 1.7927\n",
      "Epoch 9\n",
      "train_loss = 1.7808, val_loss = 1.7925\n",
      "Epoch 10\n",
      "train_loss = 1.7802, val_loss = 1.7931\n",
      "Epoch 11\n",
      "train_loss = 1.7819, val_loss = 1.7930\n",
      "Epoch 12\n",
      "train_loss = 1.7850, val_loss = 1.7935\n",
      "Epoch 13\n",
      "train_loss = 1.7805, val_loss = 1.7936\n",
      "Epoch 14\n",
      "train_loss = 1.7712, val_loss = 1.7931\n",
      "Epoch 15\n",
      "train_loss = 1.7852, val_loss = 1.7949\n",
      "Epoch 16\n",
      "train_loss = 1.7995, val_loss = 1.7924\n",
      "Epoch 17\n",
      "train_loss = 1.7999, val_loss = 1.7932\n",
      "Epoch 18\n",
      "train_loss = 1.8206, val_loss = 1.7924\n",
      "Epoch 19\n",
      "train_loss = 1.8954, val_loss = 1.7920\n",
      "Epoch 20\n",
      "train_loss = 1.8096, val_loss = 1.7940\n",
      "Epoch 21\n",
      "train_loss = 1.7956, val_loss = 1.7925\n",
      "Epoch 22\n",
      "train_loss = 1.7715, val_loss = 1.7939\n",
      "Epoch 23\n",
      "train_loss = 1.7892, val_loss = 1.7933\n",
      "Epoch 24\n",
      "train_loss = 1.7834, val_loss = 1.7938\n",
      "Epoch 25\n",
      "train_loss = 1.7675, val_loss = 1.7931\n",
      "Epoch 26\n",
      "train_loss = 1.7808, val_loss = 1.7926\n",
      "Epoch 27\n",
      "train_loss = 1.8059, val_loss = 1.7976\n",
      "Epoch 28\n",
      "train_loss = 1.7951, val_loss = 1.7932\n",
      "Epoch 29\n",
      "train_loss = 1.8074, val_loss = 1.7939\n",
      "Epoch 30\n",
      "train_loss = 1.7840, val_loss = 1.7920\n",
      "Epoch 31\n",
      "train_loss = 1.7775, val_loss = 1.7930\n",
      "Epoch 32\n",
      "train_loss = 1.7402, val_loss = 1.7948\n",
      "Epoch 33\n",
      "train_loss = 1.7567, val_loss = 1.7926\n",
      "Epoch 34\n",
      "train_loss = 1.7352, val_loss = 1.7928\n",
      "Epoch 35\n",
      "train_loss = 1.7635, val_loss = 1.7939\n",
      "Epoch 36\n",
      "train_loss = 1.8022, val_loss = 1.7913\n",
      "Epoch 37\n",
      "train_loss = 1.7623, val_loss = 1.7926\n",
      "Epoch 38\n",
      "train_loss = 1.7845, val_loss = 1.7927\n",
      "Epoch 39\n",
      "train_loss = 1.7692, val_loss = 1.7928\n",
      "Epoch 40\n",
      "train_loss = 1.7940, val_loss = 1.7917\n",
      "Epoch 41\n",
      "train_loss = 1.8060, val_loss = 1.7932\n",
      "Epoch 42\n",
      "train_loss = 1.7830, val_loss = 1.7934\n",
      "Epoch 43\n",
      "train_loss = 1.7737, val_loss = 1.7928\n",
      "Epoch 44\n",
      "train_loss = 1.7755, val_loss = 1.7961\n",
      "Epoch 45\n",
      "train_loss = 1.7911, val_loss = 1.7941\n",
      "Epoch 46\n",
      "train_loss = 1.7773, val_loss = 1.7946\n",
      "Epoch 47\n",
      "train_loss = 1.7838, val_loss = 1.7928\n",
      "Epoch 48\n",
      "train_loss = 1.7712, val_loss = 1.7930\n",
      "Epoch 49\n",
      "train_loss = 1.7709, val_loss = 1.7964\n",
      "Epoch 50\n",
      "train_loss = 1.7885, val_loss = 1.7926\n",
      "Epoch 51\n",
      "train_loss = 1.7751, val_loss = 1.7923\n",
      "Epoch 52\n",
      "train_loss = 1.7508, val_loss = 1.7951\n",
      "Epoch 53\n",
      "train_loss = 1.7777, val_loss = 1.7924\n",
      "Epoch 54\n",
      "train_loss = 1.7904, val_loss = 1.7921\n",
      "Epoch 55\n",
      "train_loss = 1.7707, val_loss = 1.7930\n",
      "Epoch 56\n",
      "train_loss = 1.7843, val_loss = 1.7912\n",
      "Epoch 57\n",
      "train_loss = 1.7689, val_loss = 1.7935\n",
      "Epoch 58\n",
      "train_loss = 1.7775, val_loss = 1.7987\n",
      "Epoch 59\n",
      "train_loss = 1.7821, val_loss = 1.7918\n",
      "Epoch 60\n",
      "train_loss = 1.8286, val_loss = 1.7917\n",
      "Epoch 61\n",
      "train_loss = 1.7554, val_loss = 1.7980\n",
      "Epoch 62\n",
      "train_loss = 1.7897, val_loss = 1.7929\n",
      "Epoch 63\n",
      "train_loss = 1.8584, val_loss = 1.7997\n",
      "Epoch 64\n",
      "train_loss = 1.7405, val_loss = 1.7951\n",
      "Epoch 65\n",
      "train_loss = 1.7745, val_loss = 1.7941\n",
      "Epoch 66\n",
      "train_loss = 1.7798, val_loss = 1.7955\n",
      "Epoch 67\n",
      "train_loss = 1.8697, val_loss = 1.7915\n",
      "Epoch 68\n",
      "train_loss = 1.7966, val_loss = 1.7939\n",
      "Epoch 69\n",
      "train_loss = 1.7802, val_loss = 1.7921\n",
      "Epoch 70\n",
      "train_loss = 1.7799, val_loss = 1.7967\n",
      "Epoch 71\n",
      "train_loss = 1.7784, val_loss = 1.7945\n",
      "Epoch 72\n",
      "train_loss = 1.7969, val_loss = 1.7983\n",
      "Epoch 73\n",
      "train_loss = 1.7825, val_loss = 1.7988\n",
      "Epoch 74\n",
      "train_loss = 1.7713, val_loss = 1.7978\n",
      "Epoch 75\n",
      "train_loss = 1.7857, val_loss = 1.7950\n",
      "Epoch 76\n",
      "train_loss = 1.7900, val_loss = 1.7981\n",
      "Epoch 77\n",
      "train_loss = 1.7616, val_loss = 1.7991\n",
      "Epoch 78\n",
      "train_loss = 1.7761, val_loss = 1.7973\n",
      "Epoch 79\n",
      "train_loss = 1.7677, val_loss = 1.7996\n",
      "Epoch 80\n",
      "train_loss = 1.7581, val_loss = 1.8098\n",
      "Epoch 81\n",
      "train_loss = 1.7749, val_loss = 1.8009\n",
      "Epoch 82\n",
      "train_loss = 1.7908, val_loss = 1.7994\n",
      "Epoch 83\n",
      "train_loss = 1.7803, val_loss = 1.8013\n",
      "Epoch 84\n",
      "train_loss = 1.7702, val_loss = 1.8068\n",
      "Epoch 85\n",
      "train_loss = 1.7714, val_loss = 1.7999\n",
      "Epoch 86\n",
      "train_loss = 1.7975, val_loss = 1.8052\n",
      "Epoch 87\n",
      "train_loss = 1.7729, val_loss = 1.7982\n",
      "Epoch 88\n",
      "train_loss = 1.7692, val_loss = 1.8025\n",
      "Epoch 89\n",
      "train_loss = 1.8112, val_loss = 1.7973\n",
      "Epoch 90\n",
      "train_loss = 1.8024, val_loss = 1.8094\n",
      "Epoch 91\n",
      "train_loss = 1.7678, val_loss = 1.8013\n",
      "Epoch 92\n",
      "train_loss = 1.7855, val_loss = 1.8013\n",
      "Epoch 93\n",
      "train_loss = 1.8088, val_loss = 1.7959\n",
      "Epoch 94\n",
      "train_loss = 1.8125, val_loss = 1.8070\n",
      "Epoch 95\n",
      "train_loss = 1.7666, val_loss = 1.8014\n",
      "Epoch 96\n",
      "train_loss = 1.8057, val_loss = 1.8025\n",
      "Epoch 97\n",
      "train_loss = 1.7747, val_loss = 1.7983\n",
      "Epoch 98\n",
      "train_loss = 1.7818, val_loss = 1.8010\n",
      "Epoch 99\n",
      "train_loss = 1.8019, val_loss = 1.8033\n",
      "Epoch 100\n",
      "train_loss = 1.8142, val_loss = 1.8122\n"
     ]
    }
   ],
   "source": [
    "mlp = NeuralNetwork(max_toks, [100, 100, 100], NUM_CLASSES, device=\"cuda\")\n",
    "training_loop(mlp, train_loader, val_loader, epochs=100, learning_rate=LEARNING_RATE, path=\"models/mlpseqwt\", weights=WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.7646\n",
      "accuracy = 0.2470\n",
      "f1 = 0.2438\n",
      "[[0.29005059 0.28236915 0.30215827 0.27906977 0.33469388 0.20512821]\n",
      " [0.35244519 0.35261708 0.30935252 0.37209302 0.32653061 0.28205128]\n",
      " [0.08937605 0.06473829 0.09352518 0.08914729 0.06938776 0.15384615]\n",
      " [0.13827993 0.13085399 0.15827338 0.12403101 0.14693878 0.20512821]\n",
      " [0.09612142 0.12809917 0.12230216 0.11627907 0.08571429 0.15384615]\n",
      " [0.03372681 0.04132231 0.01438849 0.01937984 0.03673469 0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, _, _, cmat = evaluate(model=mlp, val_dl=test_loader)\n",
    "print(cmat)\n",
    "plt.matshow(cmat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CharTokenDataset(X_train, Y_train, tokenizer=None, max_seq_length=max_toks, dtype=torch.int32)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_data = CharTokenDataset(X_val, Y_val, tokenizer=None, max_seq_length=max_toks, dtype = torch.int32)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Batch size is hard coded to 1 for accuracy purposes. \n",
    "test_data = CharTokenDataset(X_test, Y_test, tokenizer=None, max_seq_length=max_toks, dtype=torch.int32)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "train_loss = 1.4332, val_loss = 1.5807\n",
      "Epoch 2\n",
      "train_loss = 1.6706, val_loss = 1.5808\n",
      "Epoch 3\n",
      "train_loss = 1.5872, val_loss = 1.5799\n",
      "Epoch 4\n",
      "train_loss = 1.6782, val_loss = 1.5834\n",
      "Epoch 5\n",
      "train_loss = 1.6232, val_loss = 1.5815\n",
      "Epoch 6\n",
      "train_loss = 1.4581, val_loss = 1.5817\n",
      "Epoch 7\n",
      "train_loss = 1.7995, val_loss = 1.5818\n",
      "Epoch 8\n",
      "train_loss = 1.8153, val_loss = 1.5796\n",
      "Epoch 9\n",
      "train_loss = 1.7812, val_loss = 1.5806\n",
      "Epoch 10\n",
      "train_loss = 1.5804, val_loss = 1.5837\n",
      "Epoch 11\n",
      "train_loss = 1.5939, val_loss = 1.5806\n",
      "Epoch 12\n",
      "train_loss = 1.5466, val_loss = 1.5807\n",
      "Epoch 13\n",
      "train_loss = 1.7686, val_loss = 1.5803\n",
      "Epoch 14\n",
      "train_loss = 1.6050, val_loss = 1.5821\n",
      "Epoch 15\n",
      "train_loss = 1.6407, val_loss = 1.5804\n",
      "Epoch 16\n",
      "train_loss = 1.5188, val_loss = 1.5813\n",
      "Epoch 17\n",
      "train_loss = 1.5630, val_loss = 1.5801\n",
      "Epoch 18\n",
      "train_loss = 1.4813, val_loss = 1.5821\n",
      "Epoch 19\n",
      "train_loss = 1.4666, val_loss = 1.5837\n",
      "Epoch 20\n",
      "train_loss = 1.6385, val_loss = 1.5798\n",
      "Epoch 21\n",
      "train_loss = 1.8223, val_loss = 1.5821\n",
      "Epoch 22\n",
      "train_loss = 1.6593, val_loss = 1.5842\n",
      "Epoch 23\n",
      "train_loss = 1.6663, val_loss = 1.5806\n",
      "Epoch 24\n",
      "train_loss = 1.5748, val_loss = 1.5844\n",
      "Epoch 25\n",
      "train_loss = 1.5072, val_loss = 1.5824\n",
      "Epoch 26\n",
      "train_loss = 1.5222, val_loss = 1.5794\n",
      "Epoch 27\n",
      "train_loss = 1.6238, val_loss = 1.5800\n",
      "Epoch 28\n",
      "train_loss = 1.5551, val_loss = 1.5823\n",
      "Epoch 29\n",
      "train_loss = 1.6595, val_loss = 1.5829\n",
      "Epoch 30\n",
      "train_loss = 1.4155, val_loss = 1.5808\n",
      "Epoch 31\n",
      "train_loss = 1.7319, val_loss = 1.5812\n",
      "Epoch 32\n",
      "train_loss = 1.6191, val_loss = 1.5809\n",
      "Epoch 33\n",
      "train_loss = 1.4951, val_loss = 1.5831\n",
      "Epoch 34\n",
      "train_loss = 1.4384, val_loss = 1.5802\n",
      "Epoch 35\n",
      "train_loss = 1.5469, val_loss = 1.5807\n",
      "Epoch 36\n",
      "train_loss = 1.6942, val_loss = 1.5837\n",
      "Epoch 37\n",
      "train_loss = 1.6276, val_loss = 1.5801\n",
      "Epoch 38\n",
      "train_loss = 1.5334, val_loss = 1.5806\n",
      "Epoch 39\n",
      "train_loss = 1.3758, val_loss = 1.5816\n",
      "Epoch 40\n",
      "train_loss = 1.5840, val_loss = 1.5816\n",
      "Epoch 41\n",
      "train_loss = 1.8515, val_loss = 1.5804\n",
      "Epoch 42\n",
      "train_loss = 1.6003, val_loss = 1.5792\n",
      "Epoch 43\n",
      "train_loss = 1.5975, val_loss = 1.5811\n",
      "Epoch 44\n",
      "train_loss = 1.5505, val_loss = 1.5811\n",
      "Epoch 45\n",
      "train_loss = 1.4934, val_loss = 1.5802\n",
      "Epoch 46\n",
      "train_loss = 1.4758, val_loss = 1.5810\n",
      "Epoch 47\n",
      "train_loss = 1.4858, val_loss = 1.5834\n",
      "Epoch 48\n",
      "train_loss = 1.4888, val_loss = 1.5805\n",
      "Epoch 49\n",
      "train_loss = 1.5342, val_loss = 1.5819\n",
      "Epoch 50\n",
      "train_loss = 1.4921, val_loss = 1.5808\n",
      "Epoch 51\n",
      "train_loss = 1.5948, val_loss = 1.5800\n",
      "Epoch 52\n",
      "train_loss = 1.4963, val_loss = 1.5807\n",
      "Epoch 53\n",
      "train_loss = 1.4864, val_loss = 1.5796\n",
      "Epoch 54\n",
      "train_loss = 1.5403, val_loss = 1.5799\n",
      "Epoch 55\n",
      "train_loss = 1.5831, val_loss = 1.5818\n",
      "Epoch 56\n",
      "train_loss = 1.7488, val_loss = 1.5806\n",
      "Epoch 57\n",
      "train_loss = 1.4713, val_loss = 1.5817\n",
      "Epoch 58\n",
      "train_loss = 1.5399, val_loss = 1.5827\n",
      "Epoch 59\n",
      "train_loss = 1.6219, val_loss = 1.5821\n",
      "Epoch 60\n",
      "train_loss = 1.5469, val_loss = 1.5793\n",
      "Epoch 61\n",
      "train_loss = 1.6732, val_loss = 1.5828\n",
      "Epoch 62\n",
      "train_loss = 1.6050, val_loss = 1.5805\n",
      "Epoch 63\n",
      "train_loss = 1.4594, val_loss = 1.5802\n",
      "Epoch 64\n",
      "train_loss = 1.6099, val_loss = 1.5813\n",
      "Epoch 65\n",
      "train_loss = 1.5263, val_loss = 1.5833\n",
      "Epoch 66\n",
      "train_loss = 1.5565, val_loss = 1.5801\n",
      "Epoch 67\n",
      "train_loss = 1.5055, val_loss = 1.5817\n",
      "Epoch 68\n",
      "train_loss = 1.6369, val_loss = 1.5826\n",
      "Epoch 69\n",
      "train_loss = 1.3692, val_loss = 1.5811\n",
      "Epoch 70\n",
      "train_loss = 1.4739, val_loss = 1.5802\n",
      "Epoch 71\n",
      "train_loss = 1.5683, val_loss = 1.5818\n",
      "Epoch 72\n",
      "train_loss = 1.5377, val_loss = 1.5791\n",
      "Epoch 73\n",
      "train_loss = 1.6810, val_loss = 1.5826\n",
      "Epoch 74\n",
      "train_loss = 1.4471, val_loss = 1.5806\n",
      "Epoch 75\n",
      "train_loss = 1.5083, val_loss = 1.5814\n",
      "Epoch 76\n",
      "train_loss = 1.4541, val_loss = 1.5817\n",
      "Epoch 77\n",
      "train_loss = 1.7831, val_loss = 1.5814\n",
      "Epoch 78\n",
      "train_loss = 1.5115, val_loss = 1.5832\n",
      "Epoch 79\n",
      "train_loss = 1.6996, val_loss = 1.5814\n",
      "Epoch 80\n",
      "train_loss = 1.8335, val_loss = 1.5813\n",
      "Epoch 81\n",
      "train_loss = 1.5243, val_loss = 1.5794\n",
      "Epoch 82\n",
      "train_loss = 1.4632, val_loss = 1.5812\n",
      "Epoch 83\n",
      "train_loss = 1.7540, val_loss = 1.5809\n",
      "Epoch 84\n",
      "train_loss = 1.7286, val_loss = 1.5795\n",
      "Epoch 85\n",
      "train_loss = 1.6418, val_loss = 1.5821\n",
      "Epoch 86\n",
      "train_loss = 1.5541, val_loss = 1.5788\n",
      "Epoch 87\n",
      "train_loss = 1.6321, val_loss = 1.5836\n",
      "Epoch 88\n",
      "train_loss = 1.5666, val_loss = 1.5818\n",
      "Epoch 89\n",
      "train_loss = 1.6989, val_loss = 1.5821\n",
      "Epoch 90\n",
      "train_loss = 1.5081, val_loss = 1.5804\n",
      "Epoch 91\n",
      "train_loss = 1.5693, val_loss = 1.5795\n",
      "Epoch 92\n",
      "train_loss = 1.6273, val_loss = 1.5824\n",
      "Epoch 93\n",
      "train_loss = 1.5389, val_loss = 1.5809\n",
      "Epoch 94\n",
      "train_loss = 1.5795, val_loss = 1.5836\n",
      "Epoch 95\n",
      "train_loss = 1.5196, val_loss = 1.5806\n",
      "Epoch 96\n",
      "train_loss = 1.5531, val_loss = 1.5793\n",
      "Epoch 97\n",
      "train_loss = 1.5427, val_loss = 1.5826\n",
      "Epoch 98\n",
      "train_loss = 1.5399, val_loss = 1.5807\n",
      "Epoch 99\n",
      "train_loss = 1.7487, val_loss = 1.5807\n",
      "Epoch 100\n",
      "train_loss = 1.7733, val_loss = 1.5810\n",
      "Epoch 101\n",
      "train_loss = 1.7382, val_loss = 1.5826\n",
      "Epoch 102\n",
      "train_loss = 1.3739, val_loss = 1.5812\n",
      "Epoch 103\n",
      "train_loss = 1.6376, val_loss = 1.5814\n",
      "Epoch 104\n",
      "train_loss = 1.4636, val_loss = 1.5799\n",
      "Epoch 105\n",
      "train_loss = 1.5435, val_loss = 1.5800\n",
      "Epoch 106\n",
      "train_loss = 1.4776, val_loss = 1.5817\n",
      "Epoch 107\n",
      "train_loss = 1.5791, val_loss = 1.5799\n",
      "Epoch 108\n",
      "train_loss = 1.4101, val_loss = 1.5821\n",
      "Epoch 109\n",
      "train_loss = 1.5601, val_loss = 1.5810\n",
      "Epoch 110\n",
      "train_loss = 1.5698, val_loss = 1.5802\n",
      "Epoch 111\n",
      "train_loss = 1.8889, val_loss = 1.5817\n",
      "Epoch 112\n",
      "train_loss = 1.7487, val_loss = 1.5804\n",
      "Epoch 113\n",
      "train_loss = 1.4933, val_loss = 1.5817\n",
      "Epoch 114\n",
      "train_loss = 1.6940, val_loss = 1.5833\n",
      "Epoch 115\n",
      "train_loss = 1.6316, val_loss = 1.5786\n",
      "Epoch 116\n",
      "train_loss = 1.4226, val_loss = 1.5807\n",
      "Epoch 117\n",
      "train_loss = 1.6027, val_loss = 1.5850\n",
      "Epoch 118\n",
      "train_loss = 1.5505, val_loss = 1.5811\n",
      "Epoch 119\n",
      "train_loss = 1.5085, val_loss = 1.5815\n",
      "Epoch 120\n",
      "train_loss = 1.6220, val_loss = 1.5800\n",
      "Epoch 121\n",
      "train_loss = 1.6934, val_loss = 1.5809\n",
      "Epoch 122\n",
      "train_loss = 1.4888, val_loss = 1.5809\n",
      "Epoch 123\n",
      "train_loss = 1.4930, val_loss = 1.5810\n",
      "Epoch 124\n",
      "train_loss = 1.4836, val_loss = 1.5817\n",
      "Epoch 125\n",
      "train_loss = 1.5455, val_loss = 1.5791\n",
      "Epoch 126\n",
      "train_loss = 1.6462, val_loss = 1.5789\n",
      "Epoch 127\n",
      "train_loss = 1.4960, val_loss = 1.5813\n",
      "Epoch 128\n",
      "train_loss = 1.5862, val_loss = 1.5823\n",
      "Epoch 129\n",
      "train_loss = 1.6434, val_loss = 1.5816\n",
      "Epoch 130\n",
      "train_loss = 1.5120, val_loss = 1.5826\n",
      "Epoch 131\n",
      "train_loss = 1.7196, val_loss = 1.5801\n",
      "Epoch 132\n",
      "train_loss = 1.5417, val_loss = 1.5800\n",
      "Epoch 133\n",
      "train_loss = 1.5338, val_loss = 1.5812\n",
      "Epoch 134\n",
      "train_loss = 1.5194, val_loss = 1.5789\n",
      "Epoch 135\n",
      "train_loss = 1.8016, val_loss = 1.5804\n",
      "Epoch 136\n",
      "train_loss = 1.6999, val_loss = 1.5837\n",
      "Epoch 137\n",
      "train_loss = 1.6685, val_loss = 1.5824\n",
      "Epoch 138\n",
      "train_loss = 1.5817, val_loss = 1.5797\n",
      "Epoch 139\n",
      "train_loss = 1.5463, val_loss = 1.5825\n",
      "Epoch 140\n",
      "train_loss = 1.6703, val_loss = 1.5812\n",
      "Epoch 141\n",
      "train_loss = 1.5722, val_loss = 1.5813\n",
      "Epoch 142\n",
      "train_loss = 1.6316, val_loss = 1.5831\n",
      "Epoch 143\n",
      "train_loss = 1.5956, val_loss = 1.5829\n",
      "Epoch 144\n",
      "train_loss = 1.5466, val_loss = 1.5813\n",
      "Epoch 145\n",
      "train_loss = 1.5678, val_loss = 1.5831\n",
      "Epoch 146\n",
      "train_loss = 1.5677, val_loss = 1.5816\n",
      "Epoch 147\n",
      "train_loss = 1.4874, val_loss = 1.5811\n",
      "Epoch 148\n",
      "train_loss = 1.6385, val_loss = 1.5817\n",
      "Epoch 149\n",
      "train_loss = 1.4302, val_loss = 1.5799\n",
      "Epoch 150\n",
      "train_loss = 1.4992, val_loss = 1.5823\n",
      "Epoch 151\n",
      "train_loss = 1.8463, val_loss = 1.5820\n",
      "Epoch 152\n",
      "train_loss = 1.5359, val_loss = 1.5826\n",
      "Epoch 153\n",
      "train_loss = 1.5847, val_loss = 1.5814\n",
      "Epoch 154\n",
      "train_loss = 1.6702, val_loss = 1.5805\n",
      "Epoch 155\n",
      "train_loss = 1.7850, val_loss = 1.5801\n",
      "Epoch 156\n",
      "train_loss = 1.4913, val_loss = 1.5801\n",
      "Epoch 157\n",
      "train_loss = 1.5017, val_loss = 1.5790\n",
      "Epoch 158\n",
      "train_loss = 1.7711, val_loss = 1.5818\n",
      "Epoch 159\n",
      "train_loss = 1.5369, val_loss = 1.5793\n",
      "Epoch 160\n",
      "train_loss = 1.4115, val_loss = 1.5816\n",
      "Epoch 161\n",
      "train_loss = 1.4515, val_loss = 1.5808\n",
      "Epoch 162\n",
      "train_loss = 1.4910, val_loss = 1.5790\n",
      "Epoch 163\n",
      "train_loss = 1.6125, val_loss = 1.5818\n",
      "Epoch 164\n",
      "train_loss = 1.7025, val_loss = 1.5804\n",
      "Epoch 165\n",
      "train_loss = 1.5529, val_loss = 1.5817\n",
      "Epoch 166\n",
      "train_loss = 1.5874, val_loss = 1.5786\n",
      "Epoch 167\n",
      "train_loss = 1.8294, val_loss = 1.5774\n",
      "Epoch 168\n",
      "train_loss = 1.7434, val_loss = 1.5812\n",
      "Epoch 169\n",
      "train_loss = 1.5178, val_loss = 1.5799\n",
      "Epoch 170\n",
      "train_loss = 1.5873, val_loss = 1.5802\n",
      "Epoch 171\n",
      "train_loss = 1.6196, val_loss = 1.5807\n",
      "Epoch 172\n",
      "train_loss = 1.6020, val_loss = 1.5808\n",
      "Epoch 173\n",
      "train_loss = 1.5934, val_loss = 1.5805\n",
      "Epoch 174\n",
      "train_loss = 1.6264, val_loss = 1.5811\n",
      "Epoch 175\n",
      "train_loss = 1.5975, val_loss = 1.5804\n",
      "Epoch 176\n",
      "train_loss = 1.6074, val_loss = 1.5803\n",
      "Epoch 177\n",
      "train_loss = 1.7452, val_loss = 1.5824\n",
      "Epoch 178\n",
      "train_loss = 1.4985, val_loss = 1.5809\n",
      "Epoch 179\n",
      "train_loss = 1.5219, val_loss = 1.5823\n",
      "Epoch 180\n",
      "train_loss = 1.3987, val_loss = 1.5817\n",
      "Epoch 181\n",
      "train_loss = 1.5627, val_loss = 1.5810\n",
      "Epoch 182\n",
      "train_loss = 1.7359, val_loss = 1.5814\n",
      "Epoch 183\n",
      "train_loss = 1.5737, val_loss = 1.5796\n",
      "Epoch 184\n",
      "train_loss = 1.6610, val_loss = 1.5815\n",
      "Epoch 185\n",
      "train_loss = 1.5593, val_loss = 1.5813\n",
      "Epoch 186\n",
      "train_loss = 1.6664, val_loss = 1.5809\n",
      "Epoch 187\n",
      "train_loss = 1.4664, val_loss = 1.5796\n",
      "Epoch 188\n",
      "train_loss = 1.5685, val_loss = 1.5847\n",
      "Epoch 189\n",
      "train_loss = 1.4425, val_loss = 1.5841\n",
      "Epoch 190\n",
      "train_loss = 1.7189, val_loss = 1.5785\n",
      "Epoch 191\n",
      "train_loss = 1.5894, val_loss = 1.5806\n",
      "Epoch 192\n",
      "train_loss = 1.5251, val_loss = 1.5807\n",
      "Epoch 193\n",
      "train_loss = 1.5707, val_loss = 1.5807\n",
      "Epoch 194\n",
      "train_loss = 1.6551, val_loss = 1.5811\n",
      "Epoch 195\n",
      "train_loss = 1.5964, val_loss = 1.5809\n",
      "Epoch 196\n",
      "train_loss = 1.5977, val_loss = 1.5800\n",
      "Epoch 197\n",
      "train_loss = 1.7162, val_loss = 1.5808\n",
      "Epoch 198\n",
      "train_loss = 1.5252, val_loss = 1.5812\n",
      "Epoch 199\n",
      "train_loss = 1.5611, val_loss = 1.5817\n",
      "Epoch 200\n",
      "train_loss = 1.4674, val_loss = 1.5827\n",
      "Epoch 201\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Joaquin\\Desktop\\LaSalle\\MACLEARN\\MachineLearning\\emotion_charseq.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joaquin/Desktop/LaSalle/MACLEARN/MachineLearning/emotion_charseq.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lstm \u001b[39m=\u001b[39m LSTMNetwork(\u001b[39m40\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m10\u001b[39m, NUM_CLASSES)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Joaquin/Desktop/LaSalle/MACLEARN/MachineLearning/emotion_charseq.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m training_loop(lstm, train_loader, val_loader, epochs\u001b[39m=\u001b[39;49mEPOCHS, learning_rate\u001b[39m=\u001b[39;49mLEARNING_RATE, path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmodels/lstmseq\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joaquin/Desktop/LaSalle/MACLEARN/MachineLearning/emotion_charseq.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m torch\u001b[39m.\u001b[39msave(lstm\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39mmodels/lstmseq\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\Desktop\\LaSalle\\MACLEARN\\MachineLearning\\utils\\trainer.py:45\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(model, train_dl, val_dl, epochs, learning_rate, weights, path, tol, min_epoch)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> 45\u001b[0m train_loss \u001b[39m=\u001b[39m train_one_epoch(model, train_dl, optimizer, loss_fn)\n\u001b[0;32m     47\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m     48\u001b[0m running_vloss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\Desktop\\LaSalle\\MACLEARN\\MachineLearning\\utils\\trainer.py:22\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dl, optimizer, loss_fn)\u001b[0m\n\u001b[0;32m     19\u001b[0m inputs, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 22\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     23\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[39m# Compute the loss and its gradients\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\miniconda3\\envs\\agienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\Desktop\\LaSalle\\MACLEARN\\MachineLearning\\utils\\rnn.py:34\u001b[0m, in \u001b[0;36mLSTMNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m out \u001b[39m=\u001b[39m out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[0;32m     33\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out)\n\u001b[1;32m---> 34\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     35\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm = LSTMNetwork(40, 100, 10, NUM_CLASSES)\n",
    "training_loop(lstm, train_loader, val_loader, epochs=EPOCHS, learning_rate=LEARNING_RATE, path=\"models/lstmseq\")\n",
    "torch.save(lstm.state_dict(), \"models/lstmseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 1.5594\n",
      "accuracy = 0.3310\n",
      "f1 = 0.2527\n",
      "[[0.28633094 0.29352851 0.         0.33333333 0.         0.        ]\n",
      " [0.3294964  0.3559322  0.         0.66666667 0.5        0.        ]\n",
      " [0.08489209 0.0770416  0.         0.         0.         0.        ]\n",
      " [0.1381295  0.13713405 0.         0.         0.25       0.        ]\n",
      " [0.12086331 0.10708783 0.         0.         0.25       0.        ]\n",
      " [0.04028777 0.02927581 0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "_, _, _, cmat = evaluate(model=lstm, val_dl=test_loader)\n",
    "print(cmat)\n",
    "plt.matshow(cmat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 6])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (10) to match target batch_size (1).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Joaquin\\Desktop\\LaSalle\\MACLEARN\\MachineLearning\\emotion_charseq.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joaquin/Desktop/LaSalle/MACLEARN/MachineLearning/emotion_charseq.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lstm \u001b[39m=\u001b[39m LSTMNetwork(\u001b[39m40\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m10\u001b[39m, NUM_CLASSES)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Joaquin/Desktop/LaSalle/MACLEARN/MachineLearning/emotion_charseq.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m _, _, _, cmat \u001b[39m=\u001b[39m evaluate(model\u001b[39m=\u001b[39;49mlstm, val_dl\u001b[39m=\u001b[39;49mtest_loader)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joaquin/Desktop/LaSalle/MACLEARN/MachineLearning/emotion_charseq.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(cmat)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joaquin/Desktop/LaSalle/MACLEARN/MachineLearning/emotion_charseq.ipynb#X36sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mmatshow(cmat)\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\Desktop\\LaSalle\\MACLEARN\\MachineLearning\\utils\\trainer.py:83\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, val_dl, weights)\u001b[0m\n\u001b[0;32m     81\u001b[0m voutputs \u001b[39m=\u001b[39m model(vinputs)\n\u001b[0;32m     82\u001b[0m voutputs \u001b[39m=\u001b[39m voutputs\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 83\u001b[0m vloss \u001b[39m=\u001b[39m loss_fn(voutputs, vlabels)\n\u001b[0;32m     84\u001b[0m running_vloss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m vloss\n\u001b[0;32m     86\u001b[0m \u001b[39m# Use the outputs to get a decision rule via softmax. This is done to get a tangible result for \u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[39m# Accuracy and f1\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \n\u001b[0;32m     89\u001b[0m \u001b[39m# Apply a weighting to the outputs based on prior knowledge\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\miniconda3\\envs\\agienv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\miniconda3\\envs\\agienv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\miniconda3\\envs\\agienv\\Lib\\site-packages\\torch\\nn\\functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (10) to match target batch_size (1)."
     ]
    }
   ],
   "source": [
    "lstm = LSTMNetwork(40, 100, 10, NUM_CLASSES)\n",
    "_, _, _, cmat = evaluate(model=lstm, val_dl=test_loader)\n",
    "print(cmat)\n",
    "plt.matshow(cmat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CharTokenDataset(X_train, Y_train, tokenizer=None, max_seq_length=max_toks, dtype=torch.int32)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_data = CharTokenDataset(X_val, Y_val, tokenizer=None, max_seq_length=max_toks, dtype = torch.int32)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Batch size is hard coded to 1 for accuracy purposes. \n",
    "test_data = CharTokenDataset(X_test, Y_test, tokenizer=None, max_seq_length=max_toks, dtype=torch.int32)\n",
    "test_loader = DataLoader(test_data, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "train_loss = 1.6590, val_loss = 1.5858\n",
      "Epoch 2\n",
      "train_loss = 1.3555, val_loss = 1.5766\n",
      "Epoch 3\n",
      "train_loss = 1.6147, val_loss = 1.5815\n",
      "Epoch 4\n",
      "train_loss = 1.4530, val_loss = 1.5795\n",
      "Epoch 5\n",
      "train_loss = 1.6592, val_loss = 1.5803\n",
      "Epoch 6\n",
      "train_loss = 1.8088, val_loss = 1.5786\n",
      "Epoch 7\n",
      "train_loss = 1.6071, val_loss = 1.5804\n",
      "Epoch 8\n",
      "train_loss = 1.6058, val_loss = 1.5801\n",
      "Epoch 9\n",
      "train_loss = 1.5417, val_loss = 1.5810\n",
      "Epoch 10\n",
      "train_loss = 1.5391, val_loss = 1.5836\n",
      "Epoch 11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Joaquin\\Desktop\\LaSalle\\MACLEARN\\MachineLearning\\emotion_charseq.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joaquin/Desktop/LaSalle/MACLEARN/MachineLearning/emotion_charseq.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m transformer \u001b[39m=\u001b[39m TransformerEncoder(max_toks, NUM_CLASSES, \u001b[39m10\u001b[39m, \u001b[39m5\u001b[39m, \u001b[39m4\u001b[39m, ff\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, dropout\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Joaquin/Desktop/LaSalle/MACLEARN/MachineLearning/emotion_charseq.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m training_loop(transformer, train_loader, val_loader, epochs\u001b[39m=\u001b[39;49mEPOCHS, learning_rate\u001b[39m=\u001b[39;49mLEARNING_RATE)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joaquin/Desktop/LaSalle/MACLEARN/MachineLearning/emotion_charseq.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m torch\u001b[39m.\u001b[39msave(transformer\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39mmodels/xformerseq\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\Desktop\\LaSalle\\MACLEARN\\MachineLearning\\utils\\trainer.py:45\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(model, train_dl, val_dl, epochs, learning_rate, weights, path, tol, min_epoch)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> 45\u001b[0m train_loss \u001b[39m=\u001b[39m train_one_epoch(model, train_dl, optimizer, loss_fn)\n\u001b[0;32m     47\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m     48\u001b[0m running_vloss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\Desktop\\LaSalle\\MACLEARN\\MachineLearning\\utils\\trainer.py:27\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, dl, optimizer, loss_fn)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m# Compute the loss and its gradients\u001b[39;00m\n\u001b[0;32m     26\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, labels)\n\u001b[1;32m---> 27\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     29\u001b[0m \u001b[39m# Adjust learning weights\u001b[39;00m\n\u001b[0;32m     30\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\miniconda3\\envs\\agienv\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\miniconda3\\envs\\agienv\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transformer = TransformerEncoder(max_toks, NUM_CLASSES, 10, 5, 4, ff=10, dropout=0.1)\n",
    "training_loop(transformer, train_loader, val_loader, epochs=EPOCHS, learning_rate=LEARNING_RATE)\n",
    "torch.save(transformer.state_dict(), \"models/xformerseq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Joaquin\\Desktop\\LaSalle\\MACLEARN\\MachineLearning\\emotion_charseq.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Joaquin/Desktop/LaSalle/MACLEARN/MachineLearning/emotion_charseq.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m transformer \u001b[39m=\u001b[39m TransformerEncoder(max_toks, NUM_CLASSES, \u001b[39m10\u001b[39;49m, \u001b[39m5\u001b[39;49m, \u001b[39m4\u001b[39;49m, ff\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m, dropout\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joaquin/Desktop/LaSalle/MACLEARN/MachineLearning/emotion_charseq.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m _, _, _, cmat \u001b[39m=\u001b[39m evaluate(model\u001b[39m=\u001b[39mtransformer, val_dl\u001b[39m=\u001b[39mtest_loader, is_seq\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Joaquin/Desktop/LaSalle/MACLEARN/MachineLearning/emotion_charseq.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(cmat)\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\Desktop\\LaSalle\\MACLEARN\\MachineLearning\\utils\\transformer.py:36\u001b[0m, in \u001b[0;36mTransformerEncoder.__init__\u001b[1;34m(self, input_size, output_size, embed_dims, num_heads, num_layers, ff, dropout, device)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(\u001b[39m255\u001b[39m, embed_dims, \u001b[39mord\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m))\n\u001b[0;32m     35\u001b[0m \u001b[39m# Positional encoding\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_encoding \u001b[39m=\u001b[39m PositionalEncoding(embed_dims, input_size)\n\u001b[0;32m     38\u001b[0m \u001b[39m# Transformer Encoder layers\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mTransformerEncoder(\n\u001b[0;32m     40\u001b[0m     nn\u001b[39m.\u001b[39mTransformerEncoderLayer(\n\u001b[0;32m     41\u001b[0m         d_model\u001b[39m=\u001b[39membed_dims,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     num_layers\u001b[39m=\u001b[39mnum_layers\n\u001b[0;32m     47\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Joaquin\\Desktop\\LaSalle\\MACLEARN\\MachineLearning\\utils\\transformer.py:18\u001b[0m, in \u001b[0;36mPositionalEncoding.__init__\u001b[1;34m(self, d_model, max_seq_length, device)\u001b[0m\n\u001b[0;32m     16\u001b[0m pos_embed[\u001b[39m0\u001b[39m, :, \u001b[39m0\u001b[39m::\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msin(position \u001b[39m*\u001b[39m div_term)\n\u001b[0;32m     17\u001b[0m pos_embed[\u001b[39m0\u001b[39m, :, \u001b[39m1\u001b[39m::\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcos(position \u001b[39m*\u001b[39m div_term)\n\u001b[1;32m---> 18\u001b[0m pos_embed \u001b[39m=\u001b[39m pos_embed\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_embed \u001b[39m=\u001b[39m pos_embed\n\u001b[0;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m device\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "transformer = TransformerEncoder(max_toks, NUM_CLASSES, 10, 5, 4, ff=30, dropout=0.1)\n",
    "_, _, _, cmat = evaluate(model=transformer, val_dl=test_loader, is_seq=True)\n",
    "print(cmat)\n",
    "plt.matshow(cmat)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
