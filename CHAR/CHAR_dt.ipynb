{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from util.pca import perform_pca\n",
    "from util.k_folds_cross_val import k_folds_x_val, get_cmat, get_metrics\n",
    "from util.hpo import gridSearchHPO\n",
    "from util.forward_select import forward_select, forward_select_and_fit\n",
    "\n",
    "from sklearn import tree   # Decision Trees\n",
    "from sklearn import metrics\n",
    "import sklearn as skl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 6\n",
    "CLASSES = [\"sadnesss\", \"joy\", \"love\", \"anger\", \"fear\"]\n",
    "\n",
    "# Load all data\n",
    "train_data = pd.read_csv(\"data/training_char.csv\")\n",
    "test_data = pd.read_csv(\"data/test_char.csv\")\n",
    "validation_data = pd.read_csv(\"data/validation_char.csv\")\n",
    "\n",
    "# Separate X's and y's from each other\n",
    "FEATURE_COLUMNS_TRAIN = [x for x in train_data if x.startswith(\"_e\")]\n",
    "FEATURE_COLUMNS_TEST = [x for x in test_data if x.startswith(\"_e\")]\n",
    "LABEL_COLUMN = \"label\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data[FEATURE_COLUMNS_TRAIN]\n",
    "Y_train = train_data[LABEL_COLUMN]\n",
    "\n",
    "X_test = test_data[FEATURE_COLUMNS_TEST]\n",
    "Y_test = test_data[LABEL_COLUMN]\n",
    "\n",
    "X_val = validation_data[FEATURE_COLUMNS_TEST]\n",
    "Y_val = validation_data[LABEL_COLUMN]\n",
    "\n",
    "# These are used to run cross validation\n",
    "X_train_val = pd.concat([X_train, X_val]) \n",
    "Y_train_val = pd.concat([Y_train, Y_test])\n",
    "\n",
    "# These are used to run val and test for Neural Nets\n",
    "X_val_test = pd.concat([X_val, X_test])\n",
    "Y_val_test = pd.concat([Y_val, Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       _e0   _e1   _e2   _e3   _e4   _e5   _e6  _e7   _e8  _e9  ...  _e15990  \\\n",
      "0      9.0   9.0   9.0   9.0   9.0   9.0   9.0  9.0   9.0  9.0  ...      9.0   \n",
      "1      0.0   0.0  13.0   0.0   0.0  22.0  22.0  0.0   0.0  0.0  ...      0.0   \n",
      "2      4.0   3.0   0.0   1.0   1.0   5.0   5.0  6.0   8.0  6.0  ...      6.0   \n",
      "3      9.0   1.0   7.0  13.0  13.0   0.0   0.0  5.0   1.0  5.0  ...      5.0   \n",
      "4      4.0  14.0  18.0   0.0   0.0   2.0   2.0  5.0  22.0  5.0  ...      5.0   \n",
      "...    ...   ...   ...   ...   ...   ...   ...  ...   ...  ...  ...      ...   \n",
      "15995  NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN   NaN  NaN  ...      NaN   \n",
      "15996  NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN   NaN  NaN  ...      NaN   \n",
      "15997  NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN   NaN  NaN  ...      NaN   \n",
      "15998  NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN   NaN  NaN  ...      NaN   \n",
      "15999  NaN   NaN   NaN   NaN   NaN   NaN   NaN  NaN   NaN  NaN  ...      NaN   \n",
      "\n",
      "       _e15991  _e15992  _e15993  _e15994  _e15995  _e15996  _e15997  _e15998  \\\n",
      "0          9.0      9.0      9.0      9.0      9.0      9.0      9.0      9.0   \n",
      "1          0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
      "2          8.0     13.0     13.0      6.0     10.0      1.0      6.0      6.0   \n",
      "3          1.0     15.0     15.0      5.0     21.0     13.0      5.0      5.0   \n",
      "4         22.0     22.0     19.0      5.0     19.0      0.0      5.0      5.0   \n",
      "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "15995      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "15996      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "15997      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "15998      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "15999      NaN      NaN      NaN      NaN      NaN      NaN      NaN      NaN   \n",
      "\n",
      "       _e15999  \n",
      "0          9.0  \n",
      "1          0.0  \n",
      "2         11.0  \n",
      "3         14.0  \n",
      "4         15.0  \n",
      "...        ...  \n",
      "15995      NaN  \n",
      "15996      NaN  \n",
      "15997      NaN  \n",
      "15998      NaN  \n",
      "15999      NaN  \n",
      "\n",
      "[16000 rows x 16000 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\techt\\Documents\\Programming\\MachineLearning\\CHAR\\CHAR_decision_tree.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/techt/Documents/Programming/MachineLearning/CHAR/CHAR_decision_tree.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m TARGET_EXPLAINED_VARIANCE \u001b[39m=\u001b[39m \u001b[39m0.95\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/techt/Documents/Programming/MachineLearning/CHAR/CHAR_decision_tree.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pca_train, X_train_reduced \u001b[39m=\u001b[39m perform_pca(X_train, TARGET_EXPLAINED_VARIANCE)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/techt/Documents/Programming/MachineLearning/CHAR/CHAR_decision_tree.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m X_val_reduced \u001b[39m=\u001b[39m pca_train\u001b[39m.\u001b[39mtransform(X_val)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/techt/Documents/Programming/MachineLearning/CHAR/CHAR_decision_tree.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m X_test_redced \u001b[39m=\u001b[39m pca_train\u001b[39m.\u001b[39mtransform(X_test)\n",
      "File \u001b[1;32mc:\\Users\\techt\\Documents\\Programming\\MachineLearning\\CHAR\\CHAR_pca.py:10\u001b[0m, in \u001b[0;36mperform_pca\u001b[1;34m(dataset, target_variance)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39m# Need to standardize the data frirst\u001b[39;00m\n\u001b[0;32m      8\u001b[0m standardized \u001b[39m=\u001b[39m (dataset \u001b[39m-\u001b[39m dataset\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)) \u001b[39m/\u001b[39m dataset\u001b[39m.\u001b[39mstd(axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m pca\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mstandardized)\n\u001b[0;32m     11\u001b[0m dataset_reduced \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39mfit_transform(X\u001b[39m=\u001b[39mstandardized)\n\u001b[0;32m     13\u001b[0m \u001b[39mreturn\u001b[39;00m pca, dataset_reduced\n",
      "File \u001b[1;32mc:\\Users\\techt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\techt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:434\u001b[0m, in \u001b[0;36mPCA.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    418\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Fit the model with X.\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \n\u001b[0;32m    420\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39m        Returns the instance itself.\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 434\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X)\n\u001b[0;32m    435\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\techt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:483\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m issparse(X):\n\u001b[0;32m    478\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    479\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPCA does not support sparse input. See \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    480\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTruncatedSVD for a possible alternative.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    481\u001b[0m     )\n\u001b[1;32m--> 483\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    484\u001b[0m     X, dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32], ensure_2d\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy\n\u001b[0;32m    485\u001b[0m )\n\u001b[0;32m    487\u001b[0m \u001b[39m# Handle n_components==None\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_components \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\techt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:605\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    603\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    604\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 605\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[0;32m    606\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    607\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\techt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    952\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    953\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    954\u001b[0m         )\n\u001b[0;32m    956\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 957\u001b[0m         _assert_all_finite(\n\u001b[0;32m    958\u001b[0m             array,\n\u001b[0;32m    959\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    960\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    961\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    964\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    965\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\techt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    123\u001b[0m     X,\n\u001b[0;32m    124\u001b[0m     xp\u001b[39m=\u001b[39;49mxp,\n\u001b[0;32m    125\u001b[0m     allow_nan\u001b[39m=\u001b[39;49mallow_nan,\n\u001b[0;32m    126\u001b[0m     msg_dtype\u001b[39m=\u001b[39;49mmsg_dtype,\n\u001b[0;32m    127\u001b[0m     estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    128\u001b[0m     input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    129\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\techt\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    155\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    158\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     )\n\u001b[1;32m--> 171\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "TARGET_EXPLAINED_VARIANCE = 0.95\n",
    "\n",
    "pca_train, X_train_reduced = perform_pca(X_train, TARGET_EXPLAINED_VARIANCE)\n",
    "X_val_reduced = pca_train.transform(X_val)\n",
    "X_test_redced = pca_train.transform(X_test)\n",
    "X_train_val_reduced = pca_train.transform(X_train_val)\n",
    "X_val_test_reduced = pca_train.transform(X_val_test)\n",
    "\n",
    "print(f\"{pca_train.n_components_} components for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train_unreduced = X_train.to_numpy()\n",
    "X_val_unreduced = X_val.to_numpy()\n",
    "X_test_unreduced = X_test.to_numpy()\n",
    "X_train_val_unreduced = X_train_val.to_numpy()\n",
    "X_val_test_unreduced = X_val_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model = tree.DecisionTreeClassifier(\n",
    "    criterion ='entropy', \n",
    "    splitter = 'best',\n",
    "    max_depth = 256, \n",
    "    max_features = 'sqrt',\n",
    "    max_leaf_nodes = 80,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_search_space = {\n",
    "    # 'ccp_alpha':[0.1, 0.2, 0.4, 0.5],\n",
    "    # 'min_impurity_decrease':[1.0, 0.5, 1.5, 2.0], # float\n",
    "    # 'min_weight_fraction_leaf':[0.1, 0.2, 0.4, 0.5],\n",
    "    # 'min_samples_leaf':[0.1, 0.2, 0.4, 0.5], \n",
    "    # 'min_samples_split':[0.1, 0.2, 0.4, 0.5],\n",
    "    # 'class_weight': [],\n",
    "    # 'random_state': []\n",
    "    }\n",
    "\n",
    "model_dt = decision_tree_model.fit(X_train, Y_train)\n",
    "\n",
    "gridsearch_dt = gridSearchHPO(model=model_dt, search_space=dt_search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_dt.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Score: {}\".format(gridsearch_dt.best_score_))\n",
    "print(\"Best params: {}\".format(gridsearch_dt.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_model_reduced : tree.DecisionTreeClassifier = skl.base.clone(decision_tree_model)\n",
    "Xt = forward_select(decision_tree_model_reduced, X_train_val_used, Y_train_val, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
